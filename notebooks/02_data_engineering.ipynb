{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 2: Data Engineering\n",
                "\n",
                "**Purpose**: Data type casting, feature normalization, and train-test split creation.\n",
                "\n",
                "**Inputs**:\n",
                "- `cleaned_data.csv` from Notebook 1\n",
                "\n",
                "**Outputs**:\n",
                "- `engineered_train.csv` → `data/splits/`\n",
                "- `engineered_test.csv` → `data/splits/`\n",
                "- `split_metadata.json` → `results/`\n",
                "- `scaler.pkl` → `models/`\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
                "import joblib\n",
                "\n",
                "# Set random seed\n",
                "RANDOM_SEED = 42\n",
                "np.random.seed(RANDOM_SEED)\n",
                "\n",
                "# Paths\n",
                "BASE_DIR = Path('.').resolve().parent\n",
                "PROCESSED_DIR = BASE_DIR / 'data' / 'processed'\n",
                "SPLITS_DIR = BASE_DIR / 'data' / 'splits'\n",
                "MODELS_DIR = BASE_DIR / 'models'\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "FIGURES_DIR = BASE_DIR / 'figures'\n",
                "\n",
                "# Create directories\n",
                "for d in [SPLITS_DIR, MODELS_DIR, RESULTS_DIR, FIGURES_DIR]:\n",
                "    d.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Configuration\n",
                "TARGET_COLUMN = 'Class'\n",
                "TEST_SIZE = 0.2\n",
                "\n",
                "print(f\"Random Seed: {RANDOM_SEED}\")\n",
                "print(f\"Test Size: {TEST_SIZE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Cleaned Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data from Notebook 1\n",
                "input_path = PROCESSED_DIR / 'cleaned_data.csv'\n",
                "df = pd.read_csv(input_path)\n",
                "\n",
                "print(f\"Loaded data shape: {df.shape}\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(df[TARGET_COLUMN].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display first few rows\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Type Casting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check current data types\n",
                "print(\"Current Data Types:\")\n",
                "print(df.dtypes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensure target is integer\n",
                "df[TARGET_COLUMN] = df[TARGET_COLUMN].astype(int)\n",
                "\n",
                "# Ensure features are float64\n",
                "feature_cols = [col for col in df.columns if col != TARGET_COLUMN]\n",
                "for col in feature_cols:\n",
                "    df[col] = df[col].astype(np.float64)\n",
                "\n",
                "print(\"\\nData types after casting:\")\n",
                "print(df.dtypes.value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature-Target Separation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and target\n",
                "X = df.drop(columns=[TARGET_COLUMN])\n",
                "y = df[TARGET_COLUMN]\n",
                "\n",
                "print(f\"Features shape: {X.shape}\")\n",
                "print(f\"Target shape: {y.shape}\")\n",
                "print(f\"\\nFeature columns: {list(X.columns)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Stratified Train-Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stratified train-test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, \n",
                "    test_size=TEST_SIZE, \n",
                "    random_state=RANDOM_SEED, \n",
                "    stratify=y  # Maintain class ratio\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]} samples\")\n",
                "print(f\"Test set: {X_test.shape[0]} samples\")\n",
                "print(f\"\\nTraining class distribution:\")\n",
                "print(y_train.value_counts())\n",
                "print(f\"\\nTest class distribution:\")\n",
                "print(y_test.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify stratification\n",
                "train_ratio = y_train.mean() * 100\n",
                "test_ratio = y_test.mean() * 100\n",
                "\n",
                "print(f\"Training anomaly ratio: {train_ratio:.2f}%\")\n",
                "print(f\"Test anomaly ratio: {test_ratio:.2f}%\")\n",
                "print(f\"\\n✅ Stratification successful!\" if abs(train_ratio - test_ratio) < 1 else \"⚠️ Stratification mismatch!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feature Normalization (StandardScaler)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check feature distributions before scaling\n",
                "print(\"Feature Statistics (Before Scaling):\")\n",
                "print(f\"Mean range: [{X_train.mean().min():.4f}, {X_train.mean().max():.4f}]\")\n",
                "print(f\"Std range: [{X_train.std().min():.4f}, {X_train.std().max():.4f}]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply StandardScaler\n",
                "scaler = StandardScaler()\n",
                "\n",
                "# Fit on training data only\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Convert back to DataFrame\n",
                "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
                "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
                "\n",
                "print(\"\\nFeature Statistics (After Scaling):\")\n",
                "print(f\"Mean range: [{X_train_scaled.mean().min():.4f}, {X_train_scaled.mean().max():.4f}]\")\n",
                "print(f\"Std range: [{X_train_scaled.std().min():.4f}, {X_train_scaled.std().max():.4f}]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize scaling effect\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Before scaling (using first 5 features)\n",
                "sample_cols = X_train.columns[:5]\n",
                "axes[0].boxplot([X_train[col] for col in sample_cols], labels=sample_cols)\n",
                "axes[0].set_title('Before Scaling (First 5 Features)')\n",
                "axes[0].set_ylabel('Value')\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# After scaling\n",
                "axes[1].boxplot([X_train_scaled[col] for col in sample_cols], labels=sample_cols)\n",
                "axes[1].set_title('After StandardScaler (First 5 Features)')\n",
                "axes[1].set_ylabel('Standardized Value')\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'scaling_comparison.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine features with target for saving\n",
                "train_df = X_train_scaled.copy()\n",
                "train_df[TARGET_COLUMN] = y_train.values\n",
                "\n",
                "test_df = X_test_scaled.copy()\n",
                "test_df[TARGET_COLUMN] = y_test.values\n",
                "\n",
                "print(f\"Training DataFrame shape: {train_df.shape}\")\n",
                "print(f\"Test DataFrame shape: {test_df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save train/test splits\n",
                "train_path = SPLITS_DIR / 'engineered_train.csv'\n",
                "test_path = SPLITS_DIR / 'engineered_test.csv'\n",
                "\n",
                "train_df.to_csv(train_path, index=False)\n",
                "test_df.to_csv(test_path, index=False)\n",
                "\n",
                "print(f\"✅ Saved training data to: {train_path}\")\n",
                "print(f\"✅ Saved test data to: {test_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save scaler\n",
                "scaler_path = MODELS_DIR / 'scaler.pkl'\n",
                "joblib.dump(scaler, scaler_path)\n",
                "print(f\"✅ Saved scaler to: {scaler_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and save split metadata\n",
                "split_metadata = {\n",
                "    \"random_seed\": RANDOM_SEED,\n",
                "    \"test_size\": TEST_SIZE,\n",
                "    \"stratified\": True,\n",
                "    \"target_column\": TARGET_COLUMN,\n",
                "    \"training_samples\": int(len(train_df)),\n",
                "    \"test_samples\": int(len(test_df)),\n",
                "    \"training_class_distribution\": {\n",
                "        \"normal\": int((y_train == 0).sum()),\n",
                "        \"anomaly\": int((y_train == 1).sum()),\n",
                "        \"anomaly_ratio\": round(y_train.mean() * 100, 2)\n",
                "    },\n",
                "    \"test_class_distribution\": {\n",
                "        \"normal\": int((y_test == 0).sum()),\n",
                "        \"anomaly\": int((y_test == 1).sum()),\n",
                "        \"anomaly_ratio\": round(y_test.mean() * 100, 2)\n",
                "    },\n",
                "    \"scaler\": {\n",
                "        \"type\": \"StandardScaler\",\n",
                "        \"fit_on\": \"training_data_only\"\n",
                "    },\n",
                "    \"feature_count\": len(feature_cols),\n",
                "    \"feature_names\": feature_cols\n",
                "}\n",
                "\n",
                "metadata_path = RESULTS_DIR / 'split_metadata.json'\n",
                "with open(metadata_path, 'w') as f:\n",
                "    json.dump(split_metadata, f, indent=2)\n",
                "\n",
                "print(f\"✅ Saved split metadata to: {metadata_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display metadata\n",
                "print(\"\\nSplit Metadata:\")\n",
                "print(\"=\" * 50)\n",
                "print(json.dumps(split_metadata, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify saved files\n",
                "train_verify = pd.read_csv(SPLITS_DIR / 'engineered_train.csv')\n",
                "test_verify = pd.read_csv(SPLITS_DIR / 'engineered_test.csv')\n",
                "scaler_verify = joblib.load(MODELS_DIR / 'scaler.pkl')\n",
                "\n",
                "print(\"Verification:\")\n",
                "print(f\"  Training shape: {train_verify.shape}\")\n",
                "print(f\"  Test shape: {test_verify.shape}\")\n",
                "print(f\"  Scaler type: {type(scaler_verify).__name__}\")\n",
                "print(f\"  Total samples: {len(train_verify) + len(test_verify)}\")\n",
                "print(\"\\n✅ Notebook 2 Complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}