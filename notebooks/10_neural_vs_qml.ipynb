{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 10: Neural vs QML Comparison\n",
                "\n",
                "**Purpose**: Compare neural network models with quantum ML models.\n",
                "\n",
                "**Inputs**:\n",
                "- `neural_metrics.csv`\n",
                "- `qml_metrics.csv`\n",
                "\n",
                "**Outputs**:\n",
                "- Comparison visualizations ‚Üí `charts/`\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Paths\n",
                "BASE_DIR = Path('.').resolve().parent\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "CHARTS_DIR = BASE_DIR / 'charts'\n",
                "CHARTS_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "# Style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "colors = {'neural': '#e74c3c', 'qml': '#9b59b6'}\n",
                "\n",
                "print(f\"Charts will be saved to: {CHARTS_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load metrics\n",
                "neural_df = pd.read_csv(RESULTS_DIR / 'neural_metrics.csv')\n",
                "qml_df = pd.read_csv(RESULTS_DIR / 'qml_metrics.csv')\n",
                "\n",
                "neural_df['category'] = 'Neural'\n",
                "qml_df['category'] = 'QML'\n",
                "\n",
                "combined_df = pd.concat([neural_df, qml_df], ignore_index=True)\n",
                "\n",
                "print(f\"Neural models: {len(neural_df)}\")\n",
                "print(f\"QML models: {len(qml_df)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display results\n",
                "print(\"\\nNeural Models:\")\n",
                "print(neural_df[['model', 'f1_score', 'roc_auc', 'train_time']].to_string(index=False))\n",
                "print(\"\\nQML Models:\")\n",
                "qml_cols = ['model', 'f1_score', 'roc_auc', 'train_time']\n",
                "if 'n_qubits' in qml_df.columns:\n",
                "    qml_cols.append('n_qubits')\n",
                "print(qml_df[qml_cols].to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Performance Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Metric comparison\n",
                "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, metric in enumerate(metrics):\n",
                "    ax = axes[i]\n",
                "    data = combined_df.sort_values(metric, ascending=True)\n",
                "    colors_list = [colors['neural'] if c == 'Neural' else colors['qml'] \n",
                "                   for c in data['category']]\n",
                "    \n",
                "    ax.barh(data['model'], data[metric], color=colors_list)\n",
                "    ax.set_xlabel(metric.replace('_', ' ').title())\n",
                "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} Comparison')\n",
                "    ax.set_xlim([0, 1])\n",
                "\n",
                "from matplotlib.patches import Patch\n",
                "legend_elements = [Patch(facecolor=colors['neural'], label='Neural'),\n",
                "                   Patch(facecolor=colors['qml'], label='QML')]\n",
                "axes[-1].legend(handles=legend_elements, loc='center', fontsize=14)\n",
                "axes[-1].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(CHARTS_DIR / 'neural_vs_qml_metrics.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"‚úÖ Saved: neural_vs_qml_metrics.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# F1 Score comparison\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "data = combined_df.sort_values('f1_score', ascending=True)\n",
                "colors_list = [colors['neural'] if c == 'Neural' else colors['qml'] \n",
                "               for c in data['category']]\n",
                "\n",
                "bars = ax.barh(data['model'], data['f1_score'], color=colors_list)\n",
                "ax.set_xlabel('F1 Score')\n",
                "ax.set_title('F1 Score: Neural vs QML Models')\n",
                "ax.set_xlim([0, 1])\n",
                "\n",
                "for bar, val in zip(bars, data['f1_score']):\n",
                "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(CHARTS_DIR / 'neural_vs_qml_f1_score.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"‚úÖ Saved: neural_vs_qml_f1_score.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Scalability Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performance vs Training Time scatter\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "\n",
                "for category, color in colors.items():\n",
                "    if category == 'neural':\n",
                "        data = neural_df\n",
                "    else:\n",
                "        data = qml_df\n",
                "    \n",
                "    ax.scatter(data['train_time'], data['f1_score'], \n",
                "               c=color, label=category.upper(), s=100, alpha=0.7)\n",
                "    \n",
                "    for _, row in data.iterrows():\n",
                "        ax.annotate(row['model'], (row['train_time'], row['f1_score']),\n",
                "                   fontsize=8, ha='left', va='bottom')\n",
                "\n",
                "ax.set_xlabel('Training Time (seconds)')\n",
                "ax.set_ylabel('F1 Score')\n",
                "ax.set_title('Performance vs Training Time: Neural vs QML')\n",
                "ax.legend()\n",
                "ax.set_xscale('log')\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(CHARTS_DIR / 'neural_vs_qml_scalability.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"‚úÖ Saved: neural_vs_qml_scalability.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# QML-specific: Performance vs Qubits\n",
                "if 'n_qubits' in qml_df.columns:\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    ax1 = axes[0]\n",
                "    ax1.scatter(qml_df['n_qubits'], qml_df['f1_score'], s=100, c=colors['qml'])\n",
                "    for _, row in qml_df.iterrows():\n",
                "        ax1.annotate(row['model'], (row['n_qubits'], row['f1_score']), fontsize=8)\n",
                "    ax1.set_xlabel('Number of Qubits')\n",
                "    ax1.set_ylabel('F1 Score')\n",
                "    ax1.set_title('QML: Performance vs Qubit Count')\n",
                "    \n",
                "    ax2 = axes[1]\n",
                "    ax2.scatter(qml_df['n_qubits'], qml_df['train_time'], s=100, c=colors['qml'])\n",
                "    for _, row in qml_df.iterrows():\n",
                "        ax2.annotate(row['model'], (row['n_qubits'], row['train_time']), fontsize=8)\n",
                "    ax2.set_xlabel('Number of Qubits')\n",
                "    ax2.set_ylabel('Training Time (s)')\n",
                "    ax2.set_title('QML: Training Time vs Qubit Count')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(CHARTS_DIR / 'qml_qubit_analysis.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(f\"‚úÖ Saved: qml_qubit_analysis.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Best model per metric comparison\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"BEST MODEL PER METRIC: NEURAL VS QML\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
                "best_results = []\n",
                "\n",
                "for metric in metrics_to_compare:\n",
                "    # Best neural\n",
                "    best_neural_idx = neural_df[metric].idxmax()\n",
                "    best_neural = neural_df.loc[best_neural_idx]\n",
                "    \n",
                "    # Best QML\n",
                "    best_qml_idx = qml_df[metric].idxmax()\n",
                "    best_qml = qml_df.loc[best_qml_idx]\n",
                "    \n",
                "    # Determine winner\n",
                "    if best_neural[metric] > best_qml[metric]:\n",
                "        winner = 'Neural'\n",
                "        winner_model = best_neural['model']\n",
                "    else:\n",
                "        winner = 'QML'\n",
                "        winner_model = best_qml['model']\n",
                "    \n",
                "    best_results.append({\n",
                "        'Metric': metric.replace('_', ' ').title(),\n",
                "        'Best Neural': f\"{best_neural['model']} ({best_neural[metric]:.4f})\",\n",
                "        'Best QML': f\"{best_qml['model']} ({best_qml[metric]:.4f})\",\n",
                "        'Winner': f\"{winner}: {winner_model}\"\n",
                "    })\n",
                "    \n",
                "    print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
                "    print(f\"  Best Neural: {best_neural['model']} = {best_neural[metric]:.4f}\")\n",
                "    print(f\"  Best QML: {best_qml['model']} = {best_qml[metric]:.4f}\")\n",
                "    print(f\"  üèÜ WINNER: {winner} ({winner_model})\")\n",
                "\n",
                "# Training time (lower is better)\n",
                "print(f\"\\nTraining Time (lower is better):\")\n",
                "fastest_neural = neural_df.loc[neural_df['train_time'].idxmin()]\n",
                "fastest_qml = qml_df.loc[qml_df['train_time'].idxmin()]\n",
                "\n",
                "print(f\"  Fastest Neural: {fastest_neural['model']} = {fastest_neural['train_time']:.4f}s\")\n",
                "print(f\"  Fastest QML: {fastest_qml['model']} = {fastest_qml['train_time']:.4f}s\")\n",
                "\n",
                "if fastest_neural['train_time'] < fastest_qml['train_time']:\n",
                "    print(f\"  üèÜ WINNER: Neural ({fastest_neural['model']})\")\n",
                "else:\n",
                "    print(f\"  üèÜ WINNER: QML ({fastest_qml['model']})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save summary\n",
                "best_df = pd.DataFrame(best_results)\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"SUMMARY TABLE\")\n",
                "print(\"=\"*70)\n",
                "print(best_df.to_string(index=False))\n",
                "\n",
                "best_df.to_csv(RESULTS_DIR / 'neural_vs_qml_best.csv', index=False)\n",
                "print(f\"\\n‚úÖ Saved: neural_vs_qml_best.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Overall best\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"OVERALL BEST MODELS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "best_f1 = combined_df.loc[combined_df['f1_score'].idxmax()]\n",
                "best_auc = combined_df.loc[combined_df['roc_auc'].idxmax()]\n",
                "\n",
                "print(f\"\\nü•á Best by F1: {best_f1['model']} ({best_f1['category']}) = {best_f1['f1_score']:.4f}\")\n",
                "print(f\"ü•á Best by AUC: {best_auc['model']} ({best_auc['category']}) = {best_auc['roc_auc']:.4f}\")\n",
                "\n",
                "print(\"\\n‚úÖ Notebook 10 Complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}