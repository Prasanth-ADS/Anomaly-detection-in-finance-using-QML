{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 3: Feature Engineering\n",
                "\n",
                "**Purpose**: Create statistical features, interaction features, perform feature importance screening and correlation analysis.\n",
                "\n",
                "**Inputs**:\n",
                "- `engineered_train.csv` from Notebook 2\n",
                "- `engineered_test.csv` from Notebook 2\n",
                "\n",
                "**Outputs**:\n",
                "- `feature_engineered_train.csv` → `data/features/`\n",
                "- `feature_engineered_test.csv` → `data/features/`\n",
                "- `feature_report.json` → `results/`\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.feature_selection import mutual_info_classif\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seed\n",
                "RANDOM_SEED = 42\n",
                "np.random.seed(RANDOM_SEED)\n",
                "\n",
                "# Paths\n",
                "BASE_DIR = Path('.').resolve().parent\n",
                "SPLITS_DIR = BASE_DIR / 'data' / 'splits'\n",
                "FEATURES_DIR = BASE_DIR / 'data' / 'features'\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "FIGURES_DIR = BASE_DIR / 'figures'\n",
                "\n",
                "# Create directories\n",
                "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Configuration\n",
                "TARGET_COLUMN = 'Class'\n",
                "\n",
                "print(f\"Random Seed: {RANDOM_SEED}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Engineered Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load train and test data\n",
                "train_df = pd.read_csv(SPLITS_DIR / 'engineered_train.csv')\n",
                "test_df = pd.read_csv(SPLITS_DIR / 'engineered_test.csv')\n",
                "\n",
                "print(f\"Training set: {train_df.shape}\")\n",
                "print(f\"Test set: {test_df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and target\n",
                "X_train = train_df.drop(columns=[TARGET_COLUMN])\n",
                "y_train = train_df[TARGET_COLUMN]\n",
                "\n",
                "X_test = test_df.drop(columns=[TARGET_COLUMN])\n",
                "y_test = test_df[TARGET_COLUMN]\n",
                "\n",
                "original_features = list(X_train.columns)\n",
                "print(f\"Original features: {len(original_features)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Statistical Feature Creation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_statistical_features(df):\n",
                "    \"\"\"\n",
                "    Create row-wise statistical features.\n",
                "    \"\"\"\n",
                "    stats_df = pd.DataFrame(index=df.index)\n",
                "    \n",
                "    # Basic statistics across V1-V28 features\n",
                "    v_cols = [col for col in df.columns if col.startswith('V')]\n",
                "    \n",
                "    stats_df['V_mean'] = df[v_cols].mean(axis=1)\n",
                "    stats_df['V_std'] = df[v_cols].std(axis=1)\n",
                "    stats_df['V_min'] = df[v_cols].min(axis=1)\n",
                "    stats_df['V_max'] = df[v_cols].max(axis=1)\n",
                "    stats_df['V_range'] = stats_df['V_max'] - stats_df['V_min']\n",
                "    stats_df['V_skew'] = df[v_cols].skew(axis=1)\n",
                "    stats_df['V_kurtosis'] = df[v_cols].kurtosis(axis=1)\n",
                "    \n",
                "    # Quantile-based features\n",
                "    stats_df['V_q25'] = df[v_cols].quantile(0.25, axis=1)\n",
                "    stats_df['V_q75'] = df[v_cols].quantile(0.75, axis=1)\n",
                "    stats_df['V_iqr'] = stats_df['V_q75'] - stats_df['V_q25']\n",
                "    \n",
                "    # Count-based features\n",
                "    stats_df['V_positive_count'] = (df[v_cols] > 0).sum(axis=1)\n",
                "    stats_df['V_negative_count'] = (df[v_cols] < 0).sum(axis=1)\n",
                "    \n",
                "    return stats_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create statistical features\n",
                "stats_train = create_statistical_features(X_train)\n",
                "stats_test = create_statistical_features(X_test)\n",
                "\n",
                "print(f\"Statistical features created: {stats_train.shape[1]}\")\n",
                "print(f\"Features: {list(stats_train.columns)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Interaction Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_interaction_features(df, top_features, n_interactions=5):\n",
                "    \"\"\"\n",
                "    Create interaction features between top important features.\n",
                "    \"\"\"\n",
                "    interaction_df = pd.DataFrame(index=df.index)\n",
                "    \n",
                "    # Select top features for interactions\n",
                "    features_to_interact = top_features[:n_interactions]\n",
                "    \n",
                "    for i, f1 in enumerate(features_to_interact):\n",
                "        for f2 in features_to_interact[i+1:]:\n",
                "            # Multiplication\n",
                "            interaction_df[f'{f1}_x_{f2}'] = df[f1] * df[f2]\n",
                "            # Ratio (with small epsilon to avoid division by zero)\n",
                "            interaction_df[f'{f1}_div_{f2}'] = df[f1] / (df[f2] + 1e-8)\n",
                "    \n",
                "    return interaction_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First, get feature importance to select top features for interaction\n",
                "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)\n",
                "rf.fit(X_train, y_train)\n",
                "\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': X_train.columns,\n",
                "    'importance': rf.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(\"Top 10 Features by Importance:\")\n",
                "print(feature_importance.head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create interaction features using top 5 important features\n",
                "top_features = feature_importance['feature'].head(5).tolist()\n",
                "print(f\"Top features for interaction: {top_features}\")\n",
                "\n",
                "interaction_train = create_interaction_features(X_train, top_features)\n",
                "interaction_test = create_interaction_features(X_test, top_features)\n",
                "\n",
                "print(f\"\\nInteraction features created: {interaction_train.shape[1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Combine All Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine original + statistical + interaction features\n",
                "X_train_enhanced = pd.concat([X_train, stats_train, interaction_train], axis=1)\n",
                "X_test_enhanced = pd.concat([X_test, stats_test, interaction_test], axis=1)\n",
                "\n",
                "print(f\"Enhanced training features: {X_train_enhanced.shape}\")\n",
                "print(f\"Enhanced test features: {X_test_enhanced.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feature Importance Screening"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Recalculate feature importance with all features\n",
                "rf_full = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)\n",
                "rf_full.fit(X_train_enhanced, y_train)\n",
                "\n",
                "full_importance = pd.DataFrame({\n",
                "    'feature': X_train_enhanced.columns,\n",
                "    'importance': rf_full.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(\"Top 15 Features (All):\")\n",
                "print(full_importance.head(15))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature importance\n",
                "plt.figure(figsize=(12, 8))\n",
                "top_20 = full_importance.head(20)\n",
                "plt.barh(range(len(top_20)), top_20['importance'].values, color='steelblue')\n",
                "plt.yticks(range(len(top_20)), top_20['feature'].values)\n",
                "plt.xlabel('Importance')\n",
                "plt.ylabel('Feature')\n",
                "plt.title('Top 20 Feature Importances (Random Forest)')\n",
                "plt.gca().invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'feature_importance.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Correlation Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute correlation with target\n",
                "correlations = X_train_enhanced.corrwith(y_train).abs().sort_values(ascending=False)\n",
                "\n",
                "print(\"Top 15 Features by Correlation with Target:\")\n",
                "print(correlations.head(15))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmap for top features\n",
                "top_corr_features = correlations.head(15).index.tolist()\n",
                "corr_matrix = X_train_enhanced[top_corr_features].corr()\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
                "plt.title('Correlation Matrix (Top 15 Features)')\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'correlation_matrix.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Select Final Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine importance and correlation for feature selection\n",
                "selection_df = pd.DataFrame({\n",
                "    'feature': full_importance['feature'],\n",
                "    'importance_rank': range(1, len(full_importance) + 1),\n",
                "    'importance': full_importance['importance']\n",
                "})\n",
                "\n",
                "# Add correlation ranking\n",
                "corr_df = pd.DataFrame({\n",
                "    'feature': correlations.index,\n",
                "    'correlation': correlations.values,\n",
                "    'correlation_rank': range(1, len(correlations) + 1)\n",
                "})\n",
                "\n",
                "selection_df = selection_df.merge(corr_df, on='feature')\n",
                "selection_df['combined_rank'] = (selection_df['importance_rank'] + selection_df['correlation_rank']) / 2\n",
                "selection_df = selection_df.sort_values('combined_rank')\n",
                "\n",
                "print(\"Top 20 Features by Combined Ranking:\")\n",
                "print(selection_df.head(20))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select top features (keeping all for now, dimensionality reduction in Notebook 4)\n",
                "# Here we just ensure features are well-formed\n",
                "selected_features = X_train_enhanced.columns.tolist()\n",
                "\n",
                "X_train_final = X_train_enhanced[selected_features]\n",
                "X_test_final = X_test_enhanced[selected_features]\n",
                "\n",
                "print(f\"Final feature count: {len(selected_features)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine with target and save\n",
                "train_final = X_train_final.copy()\n",
                "train_final[TARGET_COLUMN] = y_train.values\n",
                "\n",
                "test_final = X_test_final.copy()\n",
                "test_final[TARGET_COLUMN] = y_test.values\n",
                "\n",
                "# Save\n",
                "train_path = FEATURES_DIR / 'feature_engineered_train.csv'\n",
                "test_path = FEATURES_DIR / 'feature_engineered_test.csv'\n",
                "\n",
                "train_final.to_csv(train_path, index=False)\n",
                "test_final.to_csv(test_path, index=False)\n",
                "\n",
                "print(f\"✅ Saved feature-engineered training data to: {train_path}\")\n",
                "print(f\"✅ Saved feature-engineered test data to: {test_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and save feature report\n",
                "feature_report = {\n",
                "    \"random_seed\": RANDOM_SEED,\n",
                "    \"original_features\": len(original_features),\n",
                "    \"statistical_features\": stats_train.shape[1],\n",
                "    \"interaction_features\": interaction_train.shape[1],\n",
                "    \"total_features\": len(selected_features),\n",
                "    \"feature_names\": selected_features,\n",
                "    \"top_10_by_importance\": full_importance.head(10).to_dict('records'),\n",
                "    \"top_10_by_correlation\": [\n",
                "        {\"feature\": f, \"correlation\": round(c, 4)} \n",
                "        for f, c in zip(correlations.head(10).index, correlations.head(10).values)\n",
                "    ],\n",
                "    \"engineering_steps\": [\n",
                "        \"Created row-wise statistical features (mean, std, skew, kurtosis, etc.)\",\n",
                "        \"Created interaction features (multiplication, division) for top 5 features\",\n",
                "        \"Computed Random Forest feature importance\",\n",
                "        \"Computed correlation with target\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "report_path = RESULTS_DIR / 'feature_report.json'\n",
                "with open(report_path, 'w') as f:\n",
                "    json.dump(feature_report, f, indent=2)\n",
                "\n",
                "print(f\"✅ Saved feature report to: {report_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify\n",
                "train_verify = pd.read_csv(FEATURES_DIR / 'feature_engineered_train.csv')\n",
                "test_verify = pd.read_csv(FEATURES_DIR / 'feature_engineered_test.csv')\n",
                "\n",
                "print(\"Verification:\")\n",
                "print(f\"  Training shape: {train_verify.shape}\")\n",
                "print(f\"  Test shape: {test_verify.shape}\")\n",
                "print(f\"  Feature columns: {len(train_verify.columns) - 1}\")\n",
                "print(\"\\n✅ Notebook 3 Complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}