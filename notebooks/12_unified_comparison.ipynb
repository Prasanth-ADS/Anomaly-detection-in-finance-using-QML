{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 12: Unified Comparison Dashboard\n",
                "\n",
                "**Purpose**: Aggregate all results and provide comprehensive analysis across Classical, Neural, and QML paradigms.\n",
                "\n",
                "**Inputs**:\n",
                "- `classical_metrics.csv`\n",
                "- `neural_metrics.csv`\n",
                "- `qml_metrics.csv`\n",
                "\n",
                "**Outputs**:\n",
                "- Final dashboard visualizations ‚Üí `charts/`\n",
                "- Summary report\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from matplotlib.patches import Patch\n",
                "import json\n",
                "\n",
                "# Paths\n",
                "BASE_DIR = Path('.').resolve().parent\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "CHARTS_DIR = BASE_DIR / 'charts'\n",
                "CHARTS_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "# Style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "COLORS = {\n",
                "    'Classical': '#3498db',\n",
                "    'Neural': '#e74c3c',\n",
                "    'QML': '#9b59b6'\n",
                "}\n",
                "\n",
                "print(f\"Charts will be saved to: {CHARTS_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load all metrics\n",
                "classical_df = pd.read_csv(RESULTS_DIR / 'classical_metrics.csv')\n",
                "neural_df = pd.read_csv(RESULTS_DIR / 'neural_metrics.csv')\n",
                "qml_df = pd.read_csv(RESULTS_DIR / 'qml_metrics.csv')\n",
                "\n",
                "classical_df['category'] = 'Classical'\n",
                "neural_df['category'] = 'Neural'\n",
                "qml_df['category'] = 'QML'\n",
                "\n",
                "# Combine all\n",
                "all_df = pd.concat([classical_df, neural_df, qml_df], ignore_index=True)\n",
                "\n",
                "print(f\"Total models: {len(all_df)}\")\n",
                "print(f\"  Classical: {len(classical_df)}\")\n",
                "print(f\"  Neural: {len(neural_df)}\")\n",
                "print(f\"  QML: {len(qml_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Complete Metrics Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display complete results\n",
                "display_cols = ['model', 'category', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'train_time']\n",
                "display_df = all_df[display_cols].sort_values('f1_score', ascending=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"COMPLETE MODEL COMPARISON - SORTED BY F1 SCORE\")\n",
                "print(\"=\"*100)\n",
                "print(display_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Metric-wise Bar Plots"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprehensive metric comparison\n",
                "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, metric in enumerate(metrics):\n",
                "    ax = axes[i]\n",
                "    data = all_df.sort_values(metric, ascending=True)\n",
                "    colors_list = [COLORS[c] for c in data['category']]\n",
                "    \n",
                "    bars = ax.barh(data['model'], data[metric], color=colors_list)\n",
                "    ax.set_xlabel(metric.replace('_', ' ').title())\n",
                "    ax.set_title(f'{metric.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
                "    ax.set_xlim([0, 1])\n",
                "    \n",
                "    # Add value labels\n",
                "    for bar, val in zip(bars, data[metric]):\n",
                "        if pd.notna(val):\n",
                "            ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
                "                   f'{val:.3f}', va='center', fontsize=7)\n",
                "\n",
                "# Legend\n",
                "legend_elements = [Patch(facecolor=color, label=cat) for cat, color in COLORS.items()]\n",
                "axes[-1].legend(handles=legend_elements, loc='center', fontsize=14)\n",
                "axes[-1].set_title('Legend', fontsize=12, fontweight='bold')\n",
                "axes[-1].axis('off')\n",
                "\n",
                "plt.suptitle('Comprehensive Model Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig(CHARTS_DIR / 'unified_metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"‚úÖ Saved: unified_metrics_comparison.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. F1 Score Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# F1 Score comparison - all models\n",
                "fig, ax = plt.subplots(figsize=(14, 8))\n",
                "\n",
                "data = all_df.sort_values('f1_score', ascending=True)\n",
                "colors_list = [COLORS[c] for c in data['category']]\n",
                "\n",
                "bars = ax.barh(data['model'], data['f1_score'], color=colors_list)\n",
                "ax.set_xlabel('F1 Score', fontsize=12)\n",
                "ax.set_title('F1 Score: All Models Comparison', fontsize=14, fontweight='bold')\n",
                "ax.set_xlim([0, 1])\n",
                "\n",
                "for bar, val in zip(bars, data['f1_score']):\n",
                "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center', fontsize=9)\n",
                "\n",
                "# Add legend\n",
                "legend_elements = [Patch(facecolor=color, label=cat) for cat, color in COLORS.items()]\n",
                "ax.legend(handles=legend_elements, loc='lower right')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(CHARTS_DIR / 'unified_f1_score.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"‚úÖ Saved: unified_f1_score.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ROC-AUC Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ROC-AUC comparison - all models\n",
                "fig, ax = plt.subplots(figsize=(14, 8))\n",
                "\n",
                "data = all_df.sort_values('roc_auc', ascending=True)\n",
                "colors_list = [COLORS[c] for c in data['category']]\n",
                "\n",
                "bars = ax.barh(data['model'], data['roc_auc'], color=colors_list)\n",
                "ax.set_xlabel('ROC-AUC', fontsize=12)\n",
                "ax.set_title('ROC-AUC: All Models Comparison', fontsize=14, fontweight='bold')\n",
                "ax.set_xlim([0, 1])\n",
                "ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
                "\n",
                "for bar, val in zip(bars, data['roc_auc']):\n",
                "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center', fontsize=9)\n",
                "\n",
                "legend_elements = [Patch(facecolor=color, label=cat) for cat, color in COLORS.items()]\n",
                "ax.legend(handles=legend_elements, loc='lower right')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(CHARTS_DIR / 'unified_roc_auc.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"‚úÖ Saved: unified_roc_auc.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Runtime Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training time comparison\n",
                "fig, ax = plt.subplots(figsize=(14, 8))\n",
                "\n",
                "data = all_df.sort_values('train_time', ascending=True)\n",
                "colors_list = [COLORS[c] for c in data['category']]\n",
                "\n",
                "ax.barh(data['model'], data['train_time'], color=colors_list)\n",
                "ax.set_xlabel('Training Time (seconds) - Log Scale', fontsize=12)\n",
                "ax.set_xscale('log')\n",
                "ax.set_title('Training Time: All Models', fontsize=14, fontweight='bold')\n",
                "\n",
                "legend_elements = [Patch(facecolor=color, label=cat) for cat, color in COLORS.items()]\n",
                "ax.legend(handles=legend_elements, loc='lower right')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(CHARTS_DIR / 'unified_training_time.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"‚úÖ Saved: unified_training_time.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Radar Plot (Best Models)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Radar plot for top models from each category\n",
                "categories = ['Classical', 'Neural', 'QML']\n",
                "\n",
                "def radar_plot(ax, metrics_values, labels, title, color):\n",
                "    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
                "    metrics_values = metrics_values + [metrics_values[0]]\n",
                "    angles = angles + [angles[0]]\n",
                "    \n",
                "    ax.plot(angles, metrics_values, 'o-', linewidth=2, color=color)\n",
                "    ax.fill(angles, metrics_values, alpha=0.25, color=color)\n",
                "    ax.set_xticks(angles[:-1])\n",
                "    ax.set_xticklabels(labels)\n",
                "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
                "    ax.set_ylim([0, 1])\n",
                "\n",
                "# Get best model from each category\n",
                "best_models = {}\n",
                "for cat in categories:\n",
                "    cat_data = all_df[all_df['category'] == cat]\n",
                "    best_idx = cat_data['f1_score'].idxmax()\n",
                "    best_models[cat] = all_df.loc[best_idx]\n",
                "\n",
                "radar_metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
                "radar_labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5), subplot_kw=dict(polar=True))\n",
                "\n",
                "for i, (cat, model_data) in enumerate(best_models.items()):\n",
                "    values = [model_data[m] for m in radar_metrics]\n",
                "    radar_plot(axes[i], values, radar_labels, \n",
                "               f'{cat}: {model_data[\"model\"]}', COLORS[cat])\n",
                "\n",
                "plt.suptitle('Best Model per Category - Multi-Metric Profile', fontsize=14, fontweight='bold', y=1.05)\n",
                "plt.tight_layout()\n",
                "plt.savefig(CHARTS_DIR / 'unified_radar_best_models.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"‚úÖ Saved: unified_radar_best_models.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Best Model per Metric"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"BEST MODEL PER METRIC - ALL CATEGORIES\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
                "best_per_metric = []\n",
                "\n",
                "for metric in metrics_to_compare:\n",
                "    # Best from each category\n",
                "    best_classical = classical_df.loc[classical_df[metric].idxmax()]\n",
                "    best_neural = neural_df.loc[neural_df[metric].idxmax()]\n",
                "    best_qml = qml_df.loc[qml_df[metric].idxmax()]\n",
                "    \n",
                "    # Overall best\n",
                "    best_idx = all_df[metric].idxmax()\n",
                "    best_overall = all_df.loc[best_idx]\n",
                "    \n",
                "    best_per_metric.append({\n",
                "        'Metric': metric.replace('_', ' ').title(),\n",
                "        'Best Classical': f\"{best_classical['model']} ({best_classical[metric]:.4f})\",\n",
                "        'Best Neural': f\"{best_neural['model']} ({best_neural[metric]:.4f})\",\n",
                "        'Best QML': f\"{best_qml['model']} ({best_qml[metric]:.4f})\",\n",
                "        'Overall Winner': f\"{best_overall['category']}: {best_overall['model']}\"\n",
                "    })\n",
                "    \n",
                "    print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
                "    print(f\"  Classical: {best_classical['model']} = {best_classical[metric]:.4f}\")\n",
                "    print(f\"  Neural: {best_neural['model']} = {best_neural[metric]:.4f}\")\n",
                "    print(f\"  QML: {best_qml['model']} = {best_qml[metric]:.4f}\")\n",
                "    print(f\"  üèÜ OVERALL WINNER: {best_overall['category']} ({best_overall['model']})\")\n",
                "\n",
                "# Fastest model (training time - lower is better)\n",
                "print(f\"\\nTraining Time (lower is better):\")\n",
                "fastest_classical = classical_df.loc[classical_df['train_time'].idxmin()]\n",
                "fastest_neural = neural_df.loc[neural_df['train_time'].idxmin()]\n",
                "fastest_qml = qml_df.loc[qml_df['train_time'].idxmin()]\n",
                "fastest_overall = all_df.loc[all_df['train_time'].idxmin()]\n",
                "\n",
                "print(f\"  Classical: {fastest_classical['model']} = {fastest_classical['train_time']:.4f}s\")\n",
                "print(f\"  Neural: {fastest_neural['model']} = {fastest_neural['train_time']:.4f}s\")\n",
                "print(f\"  QML: {fastest_qml['model']} = {fastest_qml['train_time']:.4f}s\")\n",
                "print(f\"  üèÜ FASTEST: {fastest_overall['category']} ({fastest_overall['model']})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary table\n",
                "best_df = pd.DataFrame(best_per_metric)\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SUMMARY TABLE - BEST MODEL PER METRIC\")\n",
                "print(\"=\"*80)\n",
                "print(best_df.to_string(index=False))\n",
                "\n",
                "best_df.to_csv(RESULTS_DIR / 'unified_best_per_metric.csv', index=False)\n",
                "print(f\"\\n‚úÖ Saved: unified_best_per_metric.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Final Analysis & Conclusions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"FINAL ANALYSIS & CONCLUSIONS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\nüéØ KEY FINDINGS\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "# Best overall\n",
                "best_f1_overall = all_df.loc[all_df['f1_score'].idxmax()]\n",
                "best_auc_overall = all_df.loc[all_df['roc_auc'].idxmax()]\n",
                "fastest_overall = all_df.loc[all_df['train_time'].idxmin()]\n",
                "\n",
                "print(f\"\\nü•á BEST F1 SCORE: {best_f1_overall['model']} ({best_f1_overall['category']})\")\n",
                "print(f\"   F1 = {best_f1_overall['f1_score']:.4f}\")\n",
                "\n",
                "print(f\"\\nü•á BEST ROC-AUC: {best_auc_overall['model']} ({best_auc_overall['category']})\")\n",
                "print(f\"   AUC = {best_auc_overall['roc_auc']:.4f}\")\n",
                "\n",
                "print(f\"\\n‚ö° FASTEST MODEL: {fastest_overall['model']} ({fastest_overall['category']})\")\n",
                "print(f\"   Time = {fastest_overall['train_time']:.4f}s\")\n",
                "\n",
                "# Best per category\n",
                "print(\"\\n\" + \"-\" * 40)\n",
                "print(\"BEST MODEL PER CATEGORY:\")\n",
                "for cat in categories:\n",
                "    cat_best = best_models[cat]\n",
                "    print(f\"  {cat}: {cat_best['model']} (F1: {cat_best['f1_score']:.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"HONEST QML ASSESSMENT\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\"\"\n",
                "STRENGTHS:\n",
                "‚úì Novel approach to feature space exploration\n",
                "‚úì Potential exponential speedup for specific problems\n",
                "‚úì Interesting research direction for complex patterns\n",
                "\n",
                "LIMITATIONS (Current Study):\n",
                "‚úó Limited qubit count (4 qubits) restricts expressivity\n",
                "‚úó Simulator overhead - no real quantum advantage yet\n",
                "‚úó Small dataset doesn't showcase QML strengths\n",
                "‚úó Training time significantly higher than classical\n",
                "‚úó Hyperparameter tuning is computationally expensive\n",
                "\n",
                "RECOMMENDATIONS:\n",
                "‚Üí For production: Use classical ML (Random Forest, SVM, or XGBoost)\n",
                "‚Üí For research: Explore hybrid architectures with more qubits\n",
                "‚Üí Future work: Test on larger datasets with real quantum hardware\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final summary\n",
                "final_summary = {\n",
                "    'total_models': len(all_df),\n",
                "    'models_by_category': {\n",
                "        'classical': len(classical_df),\n",
                "        'neural': len(neural_df),\n",
                "        'qml': len(qml_df)\n",
                "    },\n",
                "    'best_overall': {\n",
                "        'by_f1': {\n",
                "            'model': best_f1_overall['model'],\n",
                "            'category': best_f1_overall['category'],\n",
                "            'f1_score': float(best_f1_overall['f1_score'])\n",
                "        },\n",
                "        'by_auc': {\n",
                "            'model': best_auc_overall['model'],\n",
                "            'category': best_auc_overall['category'],\n",
                "            'roc_auc': float(best_auc_overall['roc_auc'])\n",
                "        },\n",
                "        'fastest': {\n",
                "            'model': fastest_overall['model'],\n",
                "            'category': fastest_overall['category'],\n",
                "            'train_time': float(fastest_overall['train_time'])\n",
                "        }\n",
                "    },\n",
                "    'best_per_category': {\n",
                "        cat: {\n",
                "            'model': best_models[cat]['model'],\n",
                "            'f1_score': float(best_models[cat]['f1_score']),\n",
                "            'roc_auc': float(best_models[cat]['roc_auc'])\n",
                "        } for cat in categories\n",
                "    }\n",
                "}\n",
                "\n",
                "with open(RESULTS_DIR / 'final_summary.json', 'w') as f:\n",
                "    json.dump(final_summary, f, indent=2)\n",
                "\n",
                "print(\"\\n‚úÖ Saved final summary to results/final_summary.json\")\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"‚úÖ NOTEBOOK 12 COMPLETE - BENCHMARK PIPELINE FINISHED!\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}