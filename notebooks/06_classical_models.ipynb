{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 6: Classical Models for Anomaly Detection\n",
                "\n",
                "**Purpose**: Train and evaluate classical ML models for anomaly detection.\n",
                "\n",
                "**Models**:\n",
                "1. SVM (Linear)\n",
                "2. SVM (RBF)\n",
                "3. Logistic Regression\n",
                "4. Random Forest\n",
                "5. Isolation Forest\n",
                "6. Gaussian Mixture Model (GMM)\n",
                "7. Classical Autoencoder\n",
                "8. Classical MLP\n",
                "\n",
                "**Outputs**:\n",
                "- `classical_metrics.csv` → `results/`\n",
                "- ROC curves → `figures/`\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "import time\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.svm import SVC, OneClassSVM\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
                "from sklearn.mixture import GaussianMixture\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    roc_auc_score, roc_curve, confusion_matrix\n",
                ")\n",
                "import joblib\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seed\n",
                "RANDOM_SEED = 42\n",
                "np.random.seed(RANDOM_SEED)\n",
                "\n",
                "# Paths\n",
                "BASE_DIR = Path('.').resolve().parent\n",
                "FEATURES_DIR = BASE_DIR / 'data' / 'features'\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "MODELS_DIR = BASE_DIR / 'models'\n",
                "FIGURES_DIR = BASE_DIR / 'figures'\n",
                "\n",
                "TARGET_COLUMN = 'Class'\n",
                "\n",
                "print(f\"Random Seed: {RANDOM_SEED}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data and Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load PCA data\n",
                "train_df = pd.read_csv(FEATURES_DIR / 'pca_train.csv')\n",
                "test_df = pd.read_csv(FEATURES_DIR / 'pca_test.csv')\n",
                "\n",
                "X_train = train_df.drop(columns=[TARGET_COLUMN]).values\n",
                "y_train = train_df[TARGET_COLUMN].values\n",
                "\n",
                "X_test = test_df.drop(columns=[TARGET_COLUMN]).values\n",
                "y_test = test_df[TARGET_COLUMN].values\n",
                "\n",
                "print(f\"Training: {X_train.shape}, Test: {X_test.shape}\")\n",
                "print(f\"Train class dist: {np.bincount(y_train)}\")\n",
                "print(f\"Test class dist: {np.bincount(y_test)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Default parameters (skip Notebook 5 tuning)\n",
                "classical_params = {\n",
                "    'SVM_Linear': {'C': 1, 'class_weight': 'balanced'},\n",
                "    'SVM_RBF': {'C': 1, 'gamma': 'scale', 'class_weight': 'balanced'},\n",
                "    'Logistic_Regression': {'C': 1, 'class_weight': 'balanced'},\n",
                "    'Random_Forest': {'n_estimators': 100, 'max_depth': 10, 'class_weight': 'balanced'},\n",
                "    'Isolation_Forest': {'n_estimators': 100, 'contamination': 0.05},\n",
                "    'GMM': {'n_components': 2, 'covariance_type': 'full'}\n",
                "}\n",
                "print(\"Using default parameters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Evaluation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def safe_normalize(arr):\n",
                "    \"\"\"Safely normalize array to [0,1] range, handling edge cases.\"\"\"\n",
                "    arr = np.asarray(arr, dtype=float)\n",
                "    min_val, max_val = arr.min(), arr.max()\n",
                "    if max_val - min_val < 1e-10:  # All values are the same\n",
                "        return np.full_like(arr, 0.5)\n",
                "    return (arr - min_val) / (max_val - min_val)\n",
                "\n",
                "def compute_metrics(y_true, y_pred, y_prob=None):\n",
                "    \"\"\"Compute all classification metrics.\"\"\"\n",
                "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
                "    \n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_true, y_pred),\n",
                "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
                "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
                "        'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
                "        'fpr': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
                "        'tpr': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
                "    }\n",
                "    \n",
                "    if y_prob is not None and not np.any(np.isnan(y_prob)):\n",
                "        try:\n",
                "            metrics['roc_auc'] = roc_auc_score(y_true, y_prob)\n",
                "        except:\n",
                "            metrics['roc_auc'] = 0.5\n",
                "    else:\n",
                "        metrics['roc_auc'] = 0.5\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "def train_and_evaluate(model, name, X_train, y_train, X_test, y_test, is_supervised=True):\n",
                "    \"\"\"Train model and compute metrics with timing.\"\"\"\n",
                "    print(f\"\\nTraining {name}...\")\n",
                "    \n",
                "    # Training\n",
                "    start_time = time.time()\n",
                "    if is_supervised:\n",
                "        model.fit(X_train, y_train)\n",
                "    else:\n",
                "        model.fit(X_train)\n",
                "    train_time = time.time() - start_time\n",
                "    \n",
                "    # Inference\n",
                "    start_time = time.time()\n",
                "    if is_supervised:\n",
                "        y_pred = model.predict(X_test)\n",
                "        if hasattr(model, 'predict_proba'):\n",
                "            y_prob = model.predict_proba(X_test)[:, 1]\n",
                "        elif hasattr(model, 'decision_function'):\n",
                "            y_prob = safe_normalize(model.decision_function(X_test))\n",
                "        else:\n",
                "            y_prob = None\n",
                "    else:\n",
                "        # For unsupervised models\n",
                "        raw_pred = model.predict(X_test)\n",
                "        y_pred = (raw_pred == -1).astype(int)  # -1 = anomaly for sklearn\n",
                "        if hasattr(model, 'score_samples'):\n",
                "            scores = -model.score_samples(X_test)\n",
                "            y_prob = safe_normalize(scores)\n",
                "        elif hasattr(model, 'decision_function'):\n",
                "            scores = -model.decision_function(X_test)\n",
                "            y_prob = safe_normalize(scores)\n",
                "        else:\n",
                "            y_prob = None\n",
                "    inference_time = time.time() - start_time\n",
                "    \n",
                "    # Compute metrics\n",
                "    metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "    metrics['train_time'] = train_time\n",
                "    metrics['inference_time'] = inference_time\n",
                "    metrics['model'] = name\n",
                "    \n",
                "    print(f\"  F1: {metrics['f1_score']:.4f}, ROC-AUC: {metrics.get('roc_auc', 'N/A')}\")\n",
                "    print(f\"  Train time: {train_time:.2f}s, Inference time: {inference_time:.4f}s\")\n",
                "    \n",
                "    return metrics, y_pred, y_prob"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train Classical Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Store results\n",
                "all_metrics = []\n",
                "all_predictions = {}\n",
                "all_probabilities = {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. SVM Linear\n",
                "params = classical_params.get('SVM_Linear', {})\n",
                "model = SVC(kernel='linear', probability=True, random_state=RANDOM_SEED, **params)\n",
                "metrics, y_pred, y_prob = train_and_evaluate(model, 'SVM_Linear', X_train, y_train, X_test, y_test)\n",
                "all_metrics.append(metrics)\n",
                "all_predictions['SVM_Linear'] = y_pred\n",
                "all_probabilities['SVM_Linear'] = y_prob\n",
                "joblib.dump(model, MODELS_DIR / 'svm_linear.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. SVM RBF\n",
                "params = classical_params.get('SVM_RBF', {})\n",
                "model = SVC(kernel='rbf', probability=True, random_state=RANDOM_SEED, **params)\n",
                "metrics, y_pred, y_prob = train_and_evaluate(model, 'SVM_RBF', X_train, y_train, X_test, y_test)\n",
                "all_metrics.append(metrics)\n",
                "all_predictions['SVM_RBF'] = y_pred\n",
                "all_probabilities['SVM_RBF'] = y_prob\n",
                "joblib.dump(model, MODELS_DIR / 'svm_rbf.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Logistic Regression\n",
                "params = classical_params.get('Logistic_Regression', {})\n",
                "model = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000, **params)\n",
                "metrics, y_pred, y_prob = train_and_evaluate(model, 'Logistic_Regression', X_train, y_train, X_test, y_test)\n",
                "all_metrics.append(metrics)\n",
                "all_predictions['Logistic_Regression'] = y_pred\n",
                "all_probabilities['Logistic_Regression'] = y_prob\n",
                "joblib.dump(model, MODELS_DIR / 'logistic_regression.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Random Forest\n",
                "params = classical_params.get('Random_Forest', {})\n",
                "model = RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=1, **params)\n",
                "metrics, y_pred, y_prob = train_and_evaluate(model, 'Random_Forest', X_train, y_train, X_test, y_test)\n",
                "all_metrics.append(metrics)\n",
                "all_predictions['Random_Forest'] = y_pred\n",
                "all_probabilities['Random_Forest'] = y_prob\n",
                "joblib.dump(model, MODELS_DIR / 'random_forest.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Isolation Forest\n",
                "params = classical_params.get('Isolation_Forest', {})\n",
                "model = IsolationForest(random_state=RANDOM_SEED, n_jobs=1, **params)\n",
                "metrics, y_pred, y_prob = train_and_evaluate(model, 'Isolation_Forest', X_train, y_train, X_test, y_test, is_supervised=False)\n",
                "all_metrics.append(metrics)\n",
                "all_predictions['Isolation_Forest'] = y_pred\n",
                "all_probabilities['Isolation_Forest'] = y_prob\n",
                "joblib.dump(model, MODELS_DIR / 'isolation_forest.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Gaussian Mixture Model\n",
                "print(\"\\nTraining GMM...\")\n",
                "params = classical_params.get('GMM', {})\n",
                "\n",
                "start_time = time.time()\n",
                "gmm = GaussianMixture(random_state=RANDOM_SEED, **params)\n",
                "gmm.fit(X_train)\n",
                "train_time = time.time() - start_time\n",
                "\n",
                "start_time = time.time()\n",
                "scores = gmm.score_samples(X_test)\n",
                "threshold = np.percentile(gmm.score_samples(X_train), 5)\n",
                "y_pred = (scores < threshold).astype(int)\n",
                "y_prob = safe_normalize(-scores)  # Higher score = lower anomaly probability\n",
                "inference_time = time.time() - start_time\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics['train_time'] = train_time\n",
                "metrics['inference_time'] = inference_time\n",
                "metrics['model'] = 'GMM'\n",
                "\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, ROC-AUC: {metrics.get('roc_auc', 'N/A')}\")\n",
                "\n",
                "all_metrics.append(metrics)\n",
                "all_predictions['GMM'] = y_pred\n",
                "all_probabilities['GMM'] = y_prob\n",
                "joblib.dump(gmm, MODELS_DIR / 'gmm.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Classical Autoencoder\n",
                "print(\"\\nTraining Classical Autoencoder...\")\n",
                "from sklearn.neural_network import MLPRegressor\n",
                "\n",
                "start_time = time.time()\n",
                "X_train_normal = X_train[y_train == 0]\n",
                "autoencoder = MLPRegressor(\n",
                "    hidden_layer_sizes=(16, 8, 16),\n",
                "    max_iter=200,\n",
                "    random_state=RANDOM_SEED,\n",
                "    early_stopping=True\n",
                ")\n",
                "autoencoder.fit(X_train_normal, X_train_normal)\n",
                "train_time = time.time() - start_time\n",
                "\n",
                "start_time = time.time()\n",
                "X_reconstructed = autoencoder.predict(X_test)\n",
                "reconstruction_error = np.mean((X_test - X_reconstructed) ** 2, axis=1)\n",
                "threshold = np.percentile(np.mean((X_train_normal - autoencoder.predict(X_train_normal)) ** 2, axis=1), 95)\n",
                "y_pred = (reconstruction_error > threshold).astype(int)\n",
                "y_prob = safe_normalize(reconstruction_error)\n",
                "inference_time = time.time() - start_time\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics['train_time'] = train_time\n",
                "metrics['inference_time'] = inference_time\n",
                "metrics['model'] = 'Classical_Autoencoder'\n",
                "\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, ROC-AUC: {metrics.get('roc_auc', 'N/A')}\")\n",
                "\n",
                "all_metrics.append(metrics)\n",
                "all_predictions['Classical_Autoencoder'] = y_pred\n",
                "all_probabilities['Classical_Autoencoder'] = y_prob\n",
                "joblib.dump(autoencoder, MODELS_DIR / 'classical_autoencoder.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Classical MLP\n",
                "model = MLPClassifier(\n",
                "    hidden_layer_sizes=(32, 16),\n",
                "    max_iter=200,\n",
                "    random_state=RANDOM_SEED,\n",
                "    early_stopping=True\n",
                ")\n",
                "metrics, y_pred, y_prob = train_and_evaluate(model, 'Classical_MLP', X_train, y_train, X_test, y_test)\n",
                "all_metrics.append(metrics)\n",
                "all_predictions['Classical_MLP'] = y_pred\n",
                "all_probabilities['Classical_MLP'] = y_prob\n",
                "joblib.dump(model, MODELS_DIR / 'classical_mlp.pkl')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create metrics DataFrame\n",
                "metrics_df = pd.DataFrame(all_metrics)\n",
                "\n",
                "# Reorder columns\n",
                "col_order = ['model', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'fpr', 'tpr', 'train_time', 'inference_time']\n",
                "extra_cols = [c for c in metrics_df.columns if c not in col_order]\n",
                "metrics_df = metrics_df[col_order + extra_cols]\n",
                "\n",
                "# Save\n",
                "metrics_path = RESULTS_DIR / 'classical_metrics.csv'\n",
                "metrics_df.to_csv(metrics_path, index=False)\n",
                "\n",
                "print(f\"✅ Saved classical metrics to: {metrics_path}\")\n",
                "print(\"\\nClassical Model Results:\")\n",
                "print(metrics_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ROC Curves (with NaN handling)\n",
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "for model_name, y_prob in all_probabilities.items():\n",
                "    if y_prob is not None and not np.any(np.isnan(y_prob)):\n",
                "        try:\n",
                "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
                "            auc_val = metrics_df[metrics_df['model'] == model_name]['roc_auc'].values[0]\n",
                "            plt.plot(fpr, tpr, label=f'{model_name} (AUC={auc_val:.3f})')\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {model_name} ROC curve: {e}\")\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curves - Classical Models')\n",
                "plt.legend(loc='lower right')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'roc_curves_classical.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Metric comparison bar chart\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'train_time']\n",
                "titles = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC', 'Training Time (s)']\n",
                "\n",
                "for i, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
                "    ax = axes[i]\n",
                "    values = metrics_df[metric].fillna(0.5).values  # Fill NaN with 0.5 for display\n",
                "    models = metrics_df['model'].values\n",
                "    \n",
                "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(models)))\n",
                "    bars = ax.barh(models, values, color=colors)\n",
                "    ax.set_xlabel(title)\n",
                "    ax.set_title(title)\n",
                "    \n",
                "    if metric != 'train_time':\n",
                "        ax.set_xlim([0, 1])\n",
                "    \n",
                "    for bar, val in zip(bars, values):\n",
                "        ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
                "               f'{val:.3f}', va='center', fontsize=8)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'classical_metrics_comparison.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"CLASSICAL MODELS SUMMARY\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Total models trained: {len(all_metrics)}\")\n",
                "print(f\"\\nBest by F1 Score: {metrics_df.loc[metrics_df['f1_score'].idxmax(), 'model']} ({metrics_df['f1_score'].max():.4f})\")\n",
                "print(f\"Best by ROC-AUC: {metrics_df.loc[metrics_df['roc_auc'].idxmax(), 'model']} ({metrics_df['roc_auc'].max():.4f})\")\n",
                "print(f\"Fastest training: {metrics_df.loc[metrics_df['train_time'].idxmin(), 'model']} ({metrics_df['train_time'].min():.4f}s)\")\n",
                "print(\"\\n✅ Notebook 6 Complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}