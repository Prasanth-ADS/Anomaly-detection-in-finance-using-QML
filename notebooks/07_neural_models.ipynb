{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 7: Neural Network Models for Anomaly Detection\n",
                "\n",
                "**Purpose**: Train and evaluate deep learning models with proper epoch configuration.\n",
                "\n",
                "**Models & Epoch Config**:\n",
                "| Model | Max Epochs | Patience | Expected Convergence |\n",
                "|-------|------------|----------|---------------------|\n",
                "| Deep MLP | 50 | 5 | 15-25 |\n",
                "| Deep Autoencoder | 100 | 10 | 30-50 |\n",
                "| VAE | 150 | 15 | 50-80 |\n",
                "| LSTM Autoencoder | 40 | 5 | 15-20 |\n",
                "| Deep OCNN | 40 | 5 | 15-25 |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "import time\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    roc_auc_score, roc_curve, confusion_matrix\n",
                ")\n",
                "from sklearn.model_selection import train_test_split\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "RANDOM_SEED = 42\n",
                "np.random.seed(RANDOM_SEED)\n",
                "torch.manual_seed(RANDOM_SEED)\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "BASE_DIR = Path('.').resolve().parent\n",
                "FEATURES_DIR = BASE_DIR / 'data' / 'features'\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "MODELS_DIR = BASE_DIR / 'models'\n",
                "FIGURES_DIR = BASE_DIR / 'figures'\n",
                "\n",
                "TARGET_COLUMN = 'Class'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Epoch Configuration (per Master Prompt)\n",
                "EPOCH_CONFIG = {\n",
                "    'Deep_MLP': {'max_epochs': 50, 'patience': 5, 'expected': '15-25'},\n",
                "    'Deep_Autoencoder': {'max_epochs': 100, 'patience': 10, 'expected': '30-50'},\n",
                "    'VAE': {'max_epochs': 150, 'patience': 15, 'expected': '50-80'},\n",
                "    'LSTM_Autoencoder': {'max_epochs': 40, 'patience': 5, 'expected': '15-20'},\n",
                "    'Deep_OCNN': {'max_epochs': 40, 'patience': 5, 'expected': '15-25'}\n",
                "}\n",
                "\n",
                "def safe_normalize(arr):\n",
                "    arr = np.asarray(arr, dtype=float)\n",
                "    min_val, max_val = arr.min(), arr.max()\n",
                "    if max_val - min_val < 1e-10:\n",
                "        return np.full_like(arr, 0.5)\n",
                "    return (arr - min_val) / (max_val - min_val)\n",
                "\n",
                "print(\"Epoch Configuration:\")\n",
                "for model, cfg in EPOCH_CONFIG.items():\n",
                "    print(f\"  {model}: max={cfg['max_epochs']}, patience={cfg['patience']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df = pd.read_csv(FEATURES_DIR / 'pca_train.csv')\n",
                "test_df = pd.read_csv(FEATURES_DIR / 'pca_test.csv')\n",
                "\n",
                "X_train_full = train_df.drop(columns=[TARGET_COLUMN]).values.astype(np.float32)\n",
                "y_train_full = train_df[TARGET_COLUMN].values.astype(np.float32)\n",
                "\n",
                "X_test = test_df.drop(columns=[TARGET_COLUMN]).values.astype(np.float32)\n",
                "y_test = test_df[TARGET_COLUMN].values.astype(np.float32)\n",
                "\n",
                "# Split training into train/val for early stopping\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=RANDOM_SEED\n",
                ")\n",
                "\n",
                "input_dim = X_train.shape[1]\n",
                "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 32\n",
                "\n",
                "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
                "val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
                "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "# Normal-only data for autoencoders\n",
                "X_train_normal = X_train[y_train == 0]\n",
                "X_val_normal = X_val[y_val == 0]\n",
                "normal_train_loader = DataLoader(TensorDataset(torch.tensor(X_train_normal)), batch_size=BATCH_SIZE, shuffle=True)\n",
                "normal_val_loader = DataLoader(TensorDataset(torch.tensor(X_val_normal)), batch_size=BATCH_SIZE, shuffle=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Definitions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DeepMLP(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dims=[128, 64, 32, 16], dropout=0.3):\n",
                "        super().__init__()\n",
                "        layers = []\n",
                "        prev_dim = input_dim\n",
                "        for dim in hidden_dims:\n",
                "            layers.extend([nn.Linear(prev_dim, dim), nn.ReLU(), nn.Dropout(dropout)])\n",
                "            prev_dim = dim\n",
                "        layers.extend([nn.Linear(prev_dim, 1), nn.Sigmoid()])\n",
                "        self.network = nn.Sequential(*layers)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.network(x)\n",
                "\n",
                "class DeepAutoencoder(nn.Module):\n",
                "    def __init__(self, input_dim, encoder_dims=[64, 32, 16, 8]):\n",
                "        super().__init__()\n",
                "        encoder_layers, prev_dim = [], input_dim\n",
                "        for dim in encoder_dims:\n",
                "            encoder_layers.extend([nn.Linear(prev_dim, dim), nn.ReLU()])\n",
                "            prev_dim = dim\n",
                "        self.encoder = nn.Sequential(*encoder_layers)\n",
                "        \n",
                "        decoder_dims = encoder_dims[::-1][1:] + [input_dim]\n",
                "        decoder_layers = []\n",
                "        for dim in decoder_dims[:-1]:\n",
                "            decoder_layers.extend([nn.Linear(prev_dim, dim), nn.ReLU()])\n",
                "            prev_dim = dim\n",
                "        decoder_layers.append(nn.Linear(prev_dim, decoder_dims[-1]))\n",
                "        self.decoder = nn.Sequential(*decoder_layers)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.decoder(self.encoder(x))\n",
                "\n",
                "class VAE(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dims=[32, 16], latent_dim=4):\n",
                "        super().__init__()\n",
                "        encoder_layers, prev_dim = [], input_dim\n",
                "        for dim in hidden_dims:\n",
                "            encoder_layers.extend([nn.Linear(prev_dim, dim), nn.ReLU()])\n",
                "            prev_dim = dim\n",
                "        self.encoder = nn.Sequential(*encoder_layers)\n",
                "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
                "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
                "        \n",
                "        decoder_dims = hidden_dims[::-1]\n",
                "        decoder_layers = [nn.Linear(latent_dim, decoder_dims[0]), nn.ReLU()]\n",
                "        prev_dim = decoder_dims[0]\n",
                "        for dim in decoder_dims[1:]:\n",
                "            decoder_layers.extend([nn.Linear(prev_dim, dim), nn.ReLU()])\n",
                "            prev_dim = dim\n",
                "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
                "        self.decoder = nn.Sequential(*decoder_layers)\n",
                "        \n",
                "    def encode(self, x):\n",
                "        h = self.encoder(x)\n",
                "        return self.fc_mu(h), self.fc_var(h)\n",
                "    \n",
                "    def reparameterize(self, mu, log_var):\n",
                "        std = torch.exp(0.5 * log_var)\n",
                "        return mu + torch.randn_like(std) * std\n",
                "    \n",
                "    def forward(self, x):\n",
                "        mu, log_var = self.encode(x)\n",
                "        z = self.reparameterize(mu, log_var)\n",
                "        return self.decoder(z), mu, log_var\n",
                "\n",
                "class LSTMAutoencoder(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim=32, latent_dim=16):\n",
                "        super().__init__()\n",
                "        self.input_dim = input_dim\n",
                "        self.encoder_lstm = nn.LSTM(1, hidden_dim, batch_first=True)\n",
                "        self.encoder_fc = nn.Linear(hidden_dim, latent_dim)\n",
                "        self.decoder_fc = nn.Linear(latent_dim, hidden_dim)\n",
                "        self.decoder_lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
                "        self.output_fc = nn.Linear(hidden_dim, 1)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = x.unsqueeze(-1)\n",
                "        _, (h_n, _) = self.encoder_lstm(x)\n",
                "        latent = self.encoder_fc(h_n[-1])\n",
                "        h_decoded = self.decoder_fc(latent).unsqueeze(1).repeat(1, self.input_dim, 1)\n",
                "        decoded, _ = self.decoder_lstm(h_decoded)\n",
                "        return self.output_fc(decoded).squeeze(-1)\n",
                "\n",
                "class DeepOCNN(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dims=[64, 32], output_dim=16):\n",
                "        super().__init__()\n",
                "        layers, prev_dim = [], input_dim\n",
                "        for dim in hidden_dims:\n",
                "            layers.extend([nn.Linear(prev_dim, dim), nn.ReLU(), nn.BatchNorm1d(dim)])\n",
                "            prev_dim = dim\n",
                "        layers.append(nn.Linear(prev_dim, output_dim))\n",
                "        self.network = nn.Sequential(*layers)\n",
                "        self.center = nn.Parameter(torch.zeros(output_dim), requires_grad=False)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        return self.network(x)\n",
                "    \n",
                "    def compute_score(self, x):\n",
                "        return torch.sum((self.forward(x) - self.center) ** 2, dim=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training with Early Stopping"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EarlyStopping:\n",
                "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
                "    def __init__(self, patience=5, min_delta=0, restore_best=True):\n",
                "        self.patience = patience\n",
                "        self.min_delta = min_delta\n",
                "        self.restore_best = restore_best\n",
                "        self.counter = 0\n",
                "        self.best_loss = None\n",
                "        self.best_weights = None\n",
                "        self.early_stop = False\n",
                "        self.stopped_epoch = 0\n",
                "    \n",
                "    def __call__(self, val_loss, model, epoch):\n",
                "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
                "            self.best_loss = val_loss\n",
                "            self.counter = 0\n",
                "            if self.restore_best:\n",
                "                self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
                "        else:\n",
                "            self.counter += 1\n",
                "            if self.counter >= self.patience:\n",
                "                self.early_stop = True\n",
                "                self.stopped_epoch = epoch\n",
                "                if self.restore_best and self.best_weights:\n",
                "                    model.load_state_dict(self.best_weights)\n",
                "        return self.early_stop\n",
                "\n",
                "def compute_metrics(y_true, y_pred, y_prob=None):\n",
                "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_true, y_pred),\n",
                "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
                "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
                "        'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
                "        'fpr': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
                "        'tpr': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
                "    }\n",
                "    if y_prob is not None and not np.any(np.isnan(y_prob)):\n",
                "        try:\n",
                "            metrics['roc_auc'] = roc_auc_score(y_true, y_prob)\n",
                "        except:\n",
                "            metrics['roc_auc'] = 0.5\n",
                "    else:\n",
                "        metrics['roc_auc'] = 0.5\n",
                "    return metrics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_metrics = []\n",
                "all_histories = {}\n",
                "all_probabilities = {}\n",
                "epoch_metadata = {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Deep MLP\n",
                "print(\"Training Deep MLP...\")\n",
                "cfg = EPOCH_CONFIG['Deep_MLP']\n",
                "\n",
                "model = DeepMLP(input_dim).to(device)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.BCELoss()\n",
                "early_stop = EarlyStopping(patience=cfg['patience'])\n",
                "\n",
                "train_losses, val_losses = [], []\n",
                "start_time = time.time()\n",
                "\n",
                "for epoch in range(cfg['max_epochs']):\n",
                "    # Train\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    for X_batch, y_batch in train_loader:\n",
                "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        loss = criterion(model(X_batch).squeeze(), y_batch)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    train_losses.append(epoch_loss / len(train_loader))\n",
                "    \n",
                "    # Validate\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for X_batch, y_batch in val_loader:\n",
                "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "            val_loss += criterion(model(X_batch).squeeze(), y_batch).item()\n",
                "    val_losses.append(val_loss / len(val_loader))\n",
                "    \n",
                "    if (epoch + 1) % 10 == 0:\n",
                "        print(f\"  Epoch {epoch+1}/{cfg['max_epochs']}, Train: {train_losses[-1]:.4f}, Val: {val_losses[-1]:.4f}\")\n",
                "    \n",
                "    if early_stop(val_losses[-1], model, epoch + 1):\n",
                "        print(f\"  Early stopping at epoch {epoch+1}\")\n",
                "        break\n",
                "\n",
                "train_time = time.time() - start_time\n",
                "stopped_epoch = early_stop.stopped_epoch if early_stop.early_stop else epoch + 1\n",
                "\n",
                "# Evaluate\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    y_prob = model(torch.tensor(X_test).to(device)).cpu().numpy().flatten()\n",
                "    y_pred = (y_prob > 0.5).astype(int)\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics.update({'model': 'Deep_MLP', 'train_time': train_time, 'inference_time': 0.01, 'stopped_epoch': stopped_epoch})\n",
                "all_metrics.append(metrics)\n",
                "all_histories['Deep_MLP'] = {'train': train_losses, 'val': val_losses}\n",
                "all_probabilities['Deep_MLP'] = y_prob\n",
                "epoch_metadata['Deep_MLP'] = {'max_epochs': cfg['max_epochs'], 'patience': cfg['patience'], 'stopped_epoch': stopped_epoch}\n",
                "torch.save(model.state_dict(), MODELS_DIR / 'deep_mlp.pth')\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, Stopped: {stopped_epoch}/{cfg['max_epochs']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Deep Autoencoder\n",
                "print(\"\\nTraining Deep Autoencoder...\")\n",
                "cfg = EPOCH_CONFIG['Deep_Autoencoder']\n",
                "\n",
                "model = DeepAutoencoder(input_dim).to(device)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.MSELoss()\n",
                "early_stop = EarlyStopping(patience=cfg['patience'])\n",
                "\n",
                "train_losses, val_losses = [], []\n",
                "start_time = time.time()\n",
                "\n",
                "for epoch in range(cfg['max_epochs']):\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    for batch in normal_train_loader:\n",
                "        X_batch = batch[0].to(device)\n",
                "        optimizer.zero_grad()\n",
                "        loss = criterion(model(X_batch), X_batch)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    train_losses.append(epoch_loss / len(normal_train_loader))\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for batch in normal_val_loader:\n",
                "            X_batch = batch[0].to(device)\n",
                "            val_loss += criterion(model(X_batch), X_batch).item()\n",
                "    val_losses.append(val_loss / max(len(normal_val_loader), 1))\n",
                "    \n",
                "    if (epoch + 1) % 20 == 0:\n",
                "        print(f\"  Epoch {epoch+1}/{cfg['max_epochs']}, Train: {train_losses[-1]:.6f}, Val: {val_losses[-1]:.6f}\")\n",
                "    \n",
                "    if early_stop(val_losses[-1], model, epoch + 1):\n",
                "        print(f\"  Early stopping at epoch {epoch+1}\")\n",
                "        break\n",
                "\n",
                "train_time = time.time() - start_time\n",
                "stopped_epoch = early_stop.stopped_epoch if early_stop.early_stop else epoch + 1\n",
                "\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    reconstructed = model(torch.tensor(X_test).to(device)).cpu().numpy()\n",
                "    reconstruction_error = np.mean((X_test - reconstructed) ** 2, axis=1)\n",
                "    train_reconstructed = model(torch.tensor(X_train_normal).to(device)).cpu().numpy()\n",
                "    train_error = np.mean((X_train_normal - train_reconstructed) ** 2, axis=1)\n",
                "    threshold = np.percentile(train_error, 95)\n",
                "    y_pred = (reconstruction_error > threshold).astype(int)\n",
                "    y_prob = safe_normalize(reconstruction_error)\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics.update({'model': 'Deep_Autoencoder', 'train_time': train_time, 'inference_time': 0.01, 'stopped_epoch': stopped_epoch})\n",
                "all_metrics.append(metrics)\n",
                "all_histories['Deep_Autoencoder'] = {'train': train_losses, 'val': val_losses}\n",
                "all_probabilities['Deep_Autoencoder'] = y_prob\n",
                "epoch_metadata['Deep_Autoencoder'] = {'max_epochs': cfg['max_epochs'], 'patience': cfg['patience'], 'stopped_epoch': stopped_epoch}\n",
                "torch.save(model.state_dict(), MODELS_DIR / 'deep_autoencoder.pth')\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, Stopped: {stopped_epoch}/{cfg['max_epochs']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. VAE\n",
                "print(\"\\nTraining VAE...\")\n",
                "cfg = EPOCH_CONFIG['VAE']\n",
                "\n",
                "model = VAE(input_dim).to(device)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "early_stop = EarlyStopping(patience=cfg['patience'])\n",
                "\n",
                "train_losses, val_losses = [], []\n",
                "start_time = time.time()\n",
                "\n",
                "for epoch in range(cfg['max_epochs']):\n",
                "    # KL warm-up: first 30 epochs\n",
                "    kl_weight = min(1.0, epoch / 30) * 0.1\n",
                "    \n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    for batch in normal_train_loader:\n",
                "        X_batch = batch[0].to(device)\n",
                "        optimizer.zero_grad()\n",
                "        recon, mu, log_var = model(X_batch)\n",
                "        recon_loss = nn.functional.mse_loss(recon, X_batch, reduction='sum')\n",
                "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
                "        loss = recon_loss + kl_weight * kl_loss\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    train_losses.append(epoch_loss / len(normal_train_loader.dataset))\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for batch in normal_val_loader:\n",
                "            X_batch = batch[0].to(device)\n",
                "            recon, mu, log_var = model(X_batch)\n",
                "            val_loss += nn.functional.mse_loss(recon, X_batch, reduction='sum').item()\n",
                "    val_losses.append(val_loss / max(len(normal_val_loader.dataset), 1))\n",
                "    \n",
                "    if (epoch + 1) % 30 == 0:\n",
                "        print(f\"  Epoch {epoch+1}/{cfg['max_epochs']}, Train: {train_losses[-1]:.4f}, Val: {val_losses[-1]:.4f}\")\n",
                "    \n",
                "    if early_stop(val_losses[-1], model, epoch + 1):\n",
                "        print(f\"  Early stopping at epoch {epoch+1}\")\n",
                "        break\n",
                "\n",
                "train_time = time.time() - start_time\n",
                "stopped_epoch = early_stop.stopped_epoch if early_stop.early_stop else epoch + 1\n",
                "\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    reconstructed, _, _ = model(torch.tensor(X_test).to(device))\n",
                "    reconstruction_error = np.mean((X_test - reconstructed.cpu().numpy()) ** 2, axis=1)\n",
                "    train_reconstructed, _, _ = model(torch.tensor(X_train_normal).to(device))\n",
                "    train_error = np.mean((X_train_normal - train_reconstructed.cpu().numpy()) ** 2, axis=1)\n",
                "    threshold = np.percentile(train_error, 95)\n",
                "    y_pred = (reconstruction_error > threshold).astype(int)\n",
                "    y_prob = safe_normalize(reconstruction_error)\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics.update({'model': 'VAE', 'train_time': train_time, 'inference_time': 0.01, 'stopped_epoch': stopped_epoch})\n",
                "all_metrics.append(metrics)\n",
                "all_histories['VAE'] = {'train': train_losses, 'val': val_losses}\n",
                "all_probabilities['VAE'] = y_prob\n",
                "epoch_metadata['VAE'] = {'max_epochs': cfg['max_epochs'], 'patience': cfg['patience'], 'stopped_epoch': stopped_epoch}\n",
                "torch.save(model.state_dict(), MODELS_DIR / 'vae.pth')\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, Stopped: {stopped_epoch}/{cfg['max_epochs']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. LSTM Autoencoder\n",
                "print(\"\\nTraining LSTM Autoencoder...\")\n",
                "cfg = EPOCH_CONFIG['LSTM_Autoencoder']\n",
                "\n",
                "model = LSTMAutoencoder(input_dim).to(device)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.MSELoss()\n",
                "early_stop = EarlyStopping(patience=cfg['patience'])\n",
                "\n",
                "train_losses, val_losses = [], []\n",
                "start_time = time.time()\n",
                "\n",
                "for epoch in range(cfg['max_epochs']):\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    for batch in normal_train_loader:\n",
                "        X_batch = batch[0].to(device)\n",
                "        optimizer.zero_grad()\n",
                "        loss = criterion(model(X_batch), X_batch)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    train_losses.append(epoch_loss / len(normal_train_loader))\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for batch in normal_val_loader:\n",
                "            X_batch = batch[0].to(device)\n",
                "            val_loss += criterion(model(X_batch), X_batch).item()\n",
                "    val_losses.append(val_loss / max(len(normal_val_loader), 1))\n",
                "    \n",
                "    if (epoch + 1) % 10 == 0:\n",
                "        print(f\"  Epoch {epoch+1}/{cfg['max_epochs']}, Train: {train_losses[-1]:.6f}, Val: {val_losses[-1]:.6f}\")\n",
                "    \n",
                "    if early_stop(val_losses[-1], model, epoch + 1):\n",
                "        print(f\"  Early stopping at epoch {epoch+1}\")\n",
                "        break\n",
                "\n",
                "train_time = time.time() - start_time\n",
                "stopped_epoch = early_stop.stopped_epoch if early_stop.early_stop else epoch + 1\n",
                "\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    reconstructed = model(torch.tensor(X_test).to(device)).cpu().numpy()\n",
                "    reconstruction_error = np.mean((X_test - reconstructed) ** 2, axis=1)\n",
                "    train_reconstructed = model(torch.tensor(X_train_normal).to(device)).cpu().numpy()\n",
                "    train_error = np.mean((X_train_normal - train_reconstructed) ** 2, axis=1)\n",
                "    threshold = np.percentile(train_error, 95)\n",
                "    y_pred = (reconstruction_error > threshold).astype(int)\n",
                "    y_prob = safe_normalize(reconstruction_error)\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics.update({'model': 'LSTM_Autoencoder', 'train_time': train_time, 'inference_time': 0.01, 'stopped_epoch': stopped_epoch})\n",
                "all_metrics.append(metrics)\n",
                "all_histories['LSTM_Autoencoder'] = {'train': train_losses, 'val': val_losses}\n",
                "all_probabilities['LSTM_Autoencoder'] = y_prob\n",
                "epoch_metadata['LSTM_Autoencoder'] = {'max_epochs': cfg['max_epochs'], 'patience': cfg['patience'], 'stopped_epoch': stopped_epoch}\n",
                "torch.save(model.state_dict(), MODELS_DIR / 'lstm_autoencoder.pth')\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, Stopped: {stopped_epoch}/{cfg['max_epochs']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Deep OCNN\n",
                "print(\"\\nTraining Deep OCNN...\")\n",
                "cfg = EPOCH_CONFIG['Deep_OCNN']\n",
                "\n",
                "model = DeepOCNN(input_dim).to(device)\n",
                "with torch.no_grad():\n",
                "    model.center.data = model(torch.tensor(X_train_normal[:100]).to(device)).mean(dim=0)\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "early_stop = EarlyStopping(patience=cfg['patience'])\n",
                "\n",
                "train_losses, val_losses = [], []\n",
                "start_time = time.time()\n",
                "\n",
                "for epoch in range(cfg['max_epochs']):\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    for batch in normal_train_loader:\n",
                "        X_batch = batch[0].to(device)\n",
                "        optimizer.zero_grad()\n",
                "        loss = torch.mean(model.compute_score(X_batch))\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    train_losses.append(epoch_loss / len(normal_train_loader))\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for batch in normal_val_loader:\n",
                "            X_batch = batch[0].to(device)\n",
                "            val_loss += torch.mean(model.compute_score(X_batch)).item()\n",
                "    val_losses.append(val_loss / max(len(normal_val_loader), 1))\n",
                "    \n",
                "    if (epoch + 1) % 10 == 0:\n",
                "        print(f\"  Epoch {epoch+1}/{cfg['max_epochs']}, Train: {train_losses[-1]:.4f}, Val: {val_losses[-1]:.4f}\")\n",
                "    \n",
                "    if early_stop(val_losses[-1], model, epoch + 1):\n",
                "        print(f\"  Early stopping at epoch {epoch+1}\")\n",
                "        break\n",
                "\n",
                "train_time = time.time() - start_time\n",
                "stopped_epoch = early_stop.stopped_epoch if early_stop.early_stop else epoch + 1\n",
                "\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    scores = model.compute_score(torch.tensor(X_test).to(device)).cpu().numpy()\n",
                "    train_scores = model.compute_score(torch.tensor(X_train_normal).to(device)).cpu().numpy()\n",
                "    threshold = np.percentile(train_scores, 95)\n",
                "    y_pred = (scores > threshold).astype(int)\n",
                "    y_prob = safe_normalize(scores)\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics.update({'model': 'Deep_OCNN', 'train_time': train_time, 'inference_time': 0.01, 'stopped_epoch': stopped_epoch})\n",
                "all_metrics.append(metrics)\n",
                "all_histories['Deep_OCNN'] = {'train': train_losses, 'val': val_losses}\n",
                "all_probabilities['Deep_OCNN'] = y_prob\n",
                "epoch_metadata['Deep_OCNN'] = {'max_epochs': cfg['max_epochs'], 'patience': cfg['patience'], 'stopped_epoch': stopped_epoch}\n",
                "torch.save(model.state_dict(), MODELS_DIR / 'deep_ocnn.pth')\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, Stopped: {stopped_epoch}/{cfg['max_epochs']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "metrics_df = pd.DataFrame(all_metrics)\n",
                "col_order = ['model', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'fpr', 'tpr', 'train_time', 'inference_time', 'stopped_epoch']\n",
                "metrics_df = metrics_df[[c for c in col_order if c in metrics_df.columns]]\n",
                "\n",
                "metrics_path = RESULTS_DIR / 'neural_metrics.csv'\n",
                "metrics_df.to_csv(metrics_path, index=False)\n",
                "\n",
                "# Save epoch metadata\n",
                "with open(RESULTS_DIR / 'neural_epoch_metadata.json', 'w') as f:\n",
                "    json.dump(epoch_metadata, f, indent=2)\n",
                "\n",
                "print(f\"✅ Saved metrics to: {metrics_path}\")\n",
                "print(\"\\nNeural Network Results:\")\n",
                "print(metrics_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Curves (Train vs Val)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, (name, history) in enumerate(all_histories.items()):\n",
                "    ax = axes[i]\n",
                "    ax.plot(history['train'], label='Train')\n",
                "    ax.plot(history['val'], label='Val')\n",
                "    ax.axvline(x=epoch_metadata[name]['stopped_epoch']-1, color='r', linestyle='--', alpha=0.5, label='Stopped')\n",
                "    ax.set_xlabel('Epoch')\n",
                "    ax.set_ylabel('Loss')\n",
                "    ax.set_title(f\"{name} (stopped: {epoch_metadata[name]['stopped_epoch']}/{epoch_metadata[name]['max_epochs']})\")\n",
                "    ax.legend()\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "if len(all_histories) < 6:\n",
                "    axes[-1].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'neural_training_curves.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ROC Curves\n",
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "for model_name, y_prob in all_probabilities.items():\n",
                "    if y_prob is not None and not np.any(np.isnan(y_prob)):\n",
                "        try:\n",
                "            fpr_vals, tpr_vals, _ = roc_curve(y_test, y_prob)\n",
                "            auc_val = metrics_df[metrics_df['model'] == model_name]['roc_auc'].values[0]\n",
                "            plt.plot(fpr_vals, tpr_vals, label=f'{model_name} (AUC={auc_val:.3f})')\n",
                "        except:\n",
                "            pass\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curves - Neural Network Models')\n",
                "plt.legend(loc='lower right')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'roc_curves_neural.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"NEURAL NETWORK MODELS SUMMARY (With Early Stopping)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Total models trained: {len(all_metrics)}\")\n",
                "print(f\"\\nBest by F1: {metrics_df.loc[metrics_df['f1_score'].idxmax(), 'model']} ({metrics_df['f1_score'].max():.4f})\")\n",
                "print(f\"Best by AUC: {metrics_df.loc[metrics_df['roc_auc'].idxmax(), 'model']} ({metrics_df['roc_auc'].max():.4f})\")\n",
                "print(\"\\nEpoch Summary:\")\n",
                "for name, meta in epoch_metadata.items():\n",
                "    print(f\"  {name}: {meta['stopped_epoch']}/{meta['max_epochs']} (patience={meta['patience']})\")\n",
                "print(\"\\n✅ Notebook 7 Complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}