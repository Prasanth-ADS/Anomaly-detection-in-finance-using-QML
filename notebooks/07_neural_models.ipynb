{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 7: Neural Network Models for Anomaly Detection\n",
                "\n",
                "**Purpose**: Train and evaluate deep learning models using PyTorch.\n",
                "\n",
                "**Models**:\n",
                "1. Deep MLP\n",
                "2. Deep Autoencoder\n",
                "3. Variational Autoencoder (VAE)\n",
                "4. LSTM Autoencoder\n",
                "5. Deep One-Class Neural Network\n",
                "\n",
                "**Outputs**:\n",
                "- `neural_metrics.csv` → `results/`\n",
                "- Training curves → `figures/`\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "import time\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    roc_auc_score, roc_curve, confusion_matrix\n",
                ")\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seeds\n",
                "RANDOM_SEED = 42\n",
                "np.random.seed(RANDOM_SEED)\n",
                "torch.manual_seed(RANDOM_SEED)\n",
                "\n",
                "# Device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "# Paths\n",
                "BASE_DIR = Path('.').resolve().parent\n",
                "FEATURES_DIR = BASE_DIR / 'data' / 'features'\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "MODELS_DIR = BASE_DIR / 'models'\n",
                "FIGURES_DIR = BASE_DIR / 'figures'\n",
                "\n",
                "TARGET_COLUMN = 'Class'\n",
                "\n",
                "print(f\"Random Seed: {RANDOM_SEED}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Utility function to safely normalize arrays\n",
                "def safe_normalize(arr):\n",
                "    \"\"\"Safely normalize array to [0,1] range, handling edge cases.\"\"\"\n",
                "    arr = np.asarray(arr, dtype=float)\n",
                "    min_val, max_val = arr.min(), arr.max()\n",
                "    if max_val - min_val < 1e-10:\n",
                "        return np.full_like(arr, 0.5)\n",
                "    return (arr - min_val) / (max_val - min_val)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load PCA data\n",
                "train_df = pd.read_csv(FEATURES_DIR / 'pca_train.csv')\n",
                "test_df = pd.read_csv(FEATURES_DIR / 'pca_test.csv')\n",
                "\n",
                "X_train = train_df.drop(columns=[TARGET_COLUMN]).values.astype(np.float32)\n",
                "y_train = train_df[TARGET_COLUMN].values.astype(np.float32)\n",
                "\n",
                "X_test = test_df.drop(columns=[TARGET_COLUMN]).values.astype(np.float32)\n",
                "y_test = test_df[TARGET_COLUMN].values.astype(np.float32)\n",
                "\n",
                "input_dim = X_train.shape[1]\n",
                "\n",
                "print(f\"Training: {X_train.shape}, Test: {X_test.shape}\")\n",
                "print(f\"Input dimension: {input_dim}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create DataLoaders\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
                "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "# Normal-only data for autoencoder training\n",
                "X_train_normal = X_train[y_train == 0]\n",
                "normal_dataset = TensorDataset(torch.tensor(X_train_normal))\n",
                "normal_loader = DataLoader(normal_dataset, batch_size=BATCH_SIZE, shuffle=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Neural Network Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Deep MLP Classifier\n",
                "class DeepMLP(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dims=[128, 64, 32, 16], dropout=0.3):\n",
                "        super().__init__()\n",
                "        layers = []\n",
                "        prev_dim = input_dim\n",
                "        for dim in hidden_dims:\n",
                "            layers.extend([\n",
                "                nn.Linear(prev_dim, dim),\n",
                "                nn.ReLU(),\n",
                "                nn.Dropout(dropout)\n",
                "            ])\n",
                "            prev_dim = dim\n",
                "        layers.append(nn.Linear(prev_dim, 1))\n",
                "        layers.append(nn.Sigmoid())\n",
                "        self.network = nn.Sequential(*layers)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.network(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Deep Autoencoder\n",
                "class DeepAutoencoder(nn.Module):\n",
                "    def __init__(self, input_dim, encoder_dims=[64, 32, 16, 8]):\n",
                "        super().__init__()\n",
                "        encoder_layers = []\n",
                "        prev_dim = input_dim\n",
                "        for dim in encoder_dims:\n",
                "            encoder_layers.extend([nn.Linear(prev_dim, dim), nn.ReLU()])\n",
                "            prev_dim = dim\n",
                "        self.encoder = nn.Sequential(*encoder_layers)\n",
                "        \n",
                "        decoder_dims = encoder_dims[::-1][1:] + [input_dim]\n",
                "        decoder_layers = []\n",
                "        for dim in decoder_dims[:-1]:\n",
                "            decoder_layers.extend([nn.Linear(prev_dim, dim), nn.ReLU()])\n",
                "            prev_dim = dim\n",
                "        decoder_layers.append(nn.Linear(prev_dim, decoder_dims[-1]))\n",
                "        self.decoder = nn.Sequential(*decoder_layers)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.decoder(self.encoder(x))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Variational Autoencoder (VAE)\n",
                "class VAE(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dims=[32, 16], latent_dim=4):\n",
                "        super().__init__()\n",
                "        encoder_layers = []\n",
                "        prev_dim = input_dim\n",
                "        for dim in hidden_dims:\n",
                "            encoder_layers.extend([nn.Linear(prev_dim, dim), nn.ReLU()])\n",
                "            prev_dim = dim\n",
                "        self.encoder = nn.Sequential(*encoder_layers)\n",
                "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
                "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
                "        \n",
                "        decoder_dims = hidden_dims[::-1]\n",
                "        decoder_layers = [nn.Linear(latent_dim, decoder_dims[0]), nn.ReLU()]\n",
                "        prev_dim = decoder_dims[0]\n",
                "        for dim in decoder_dims[1:]:\n",
                "            decoder_layers.extend([nn.Linear(prev_dim, dim), nn.ReLU()])\n",
                "            prev_dim = dim\n",
                "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
                "        self.decoder = nn.Sequential(*decoder_layers)\n",
                "        \n",
                "    def encode(self, x):\n",
                "        h = self.encoder(x)\n",
                "        return self.fc_mu(h), self.fc_var(h)\n",
                "    \n",
                "    def reparameterize(self, mu, log_var):\n",
                "        std = torch.exp(0.5 * log_var)\n",
                "        return mu + torch.randn_like(std) * std\n",
                "    \n",
                "    def forward(self, x):\n",
                "        mu, log_var = self.encode(x)\n",
                "        z = self.reparameterize(mu, log_var)\n",
                "        return self.decoder(z), mu, log_var"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. LSTM Autoencoder\n",
                "class LSTMAutoencoder(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim=32, latent_dim=16):\n",
                "        super().__init__()\n",
                "        self.input_dim = input_dim\n",
                "        self.encoder_lstm = nn.LSTM(1, hidden_dim, batch_first=True)\n",
                "        self.encoder_fc = nn.Linear(hidden_dim, latent_dim)\n",
                "        self.decoder_fc = nn.Linear(latent_dim, hidden_dim)\n",
                "        self.decoder_lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
                "        self.output_fc = nn.Linear(hidden_dim, 1)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = x.unsqueeze(-1)\n",
                "        _, (h_n, _) = self.encoder_lstm(x)\n",
                "        latent = self.encoder_fc(h_n[-1])\n",
                "        h_decoded = self.decoder_fc(latent).unsqueeze(1).repeat(1, self.input_dim, 1)\n",
                "        decoded, _ = self.decoder_lstm(h_decoded)\n",
                "        return self.output_fc(decoded).squeeze(-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Deep One-Class Neural Network\n",
                "class DeepOCNN(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dims=[64, 32], output_dim=16):\n",
                "        super().__init__()\n",
                "        layers = []\n",
                "        prev_dim = input_dim\n",
                "        for dim in hidden_dims:\n",
                "            layers.extend([nn.Linear(prev_dim, dim), nn.ReLU(), nn.BatchNorm1d(dim)])\n",
                "            prev_dim = dim\n",
                "        layers.append(nn.Linear(prev_dim, output_dim))\n",
                "        self.network = nn.Sequential(*layers)\n",
                "        self.center = nn.Parameter(torch.zeros(output_dim), requires_grad=False)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        return self.network(x)\n",
                "    \n",
                "    def compute_score(self, x):\n",
                "        return torch.sum((self.forward(x) - self.center) ** 2, dim=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_classifier(model, train_loader, epochs=100, lr=0.001):\n",
                "    model.to(device)\n",
                "    criterion = nn.BCELoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "    history = []\n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        epoch_loss = 0\n",
                "        for X_batch, y_batch in train_loader:\n",
                "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "            optimizer.zero_grad()\n",
                "            loss = criterion(model(X_batch).squeeze(), y_batch)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            epoch_loss += loss.item()\n",
                "        history.append(epoch_loss / len(train_loader))\n",
                "        if (epoch + 1) % 20 == 0:\n",
                "            print(f\"  Epoch {epoch+1}/{epochs}, Loss: {history[-1]:.4f}\")\n",
                "    return history\n",
                "\n",
                "def train_autoencoder(model, normal_loader, epochs=100, lr=0.001):\n",
                "    model.to(device)\n",
                "    criterion = nn.MSELoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "    history = []\n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        epoch_loss = 0\n",
                "        for batch in normal_loader:\n",
                "            X_batch = batch[0].to(device)\n",
                "            optimizer.zero_grad()\n",
                "            loss = criterion(model(X_batch), X_batch)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            epoch_loss += loss.item()\n",
                "        history.append(epoch_loss / len(normal_loader))\n",
                "        if (epoch + 1) % 20 == 0:\n",
                "            print(f\"  Epoch {epoch+1}/{epochs}, Loss: {history[-1]:.6f}\")\n",
                "    return history\n",
                "\n",
                "def train_vae(model, normal_loader, epochs=100, lr=0.001, kl_weight=0.1):\n",
                "    model.to(device)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "    history = []\n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        epoch_loss = 0\n",
                "        for batch in normal_loader:\n",
                "            X_batch = batch[0].to(device)\n",
                "            optimizer.zero_grad()\n",
                "            recon, mu, log_var = model(X_batch)\n",
                "            recon_loss = nn.functional.mse_loss(recon, X_batch, reduction='sum')\n",
                "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
                "            loss = recon_loss + kl_weight * kl_loss\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            epoch_loss += loss.item()\n",
                "        history.append(epoch_loss / len(normal_loader.dataset))\n",
                "        if (epoch + 1) % 20 == 0:\n",
                "            print(f\"  Epoch {epoch+1}/{epochs}, Loss: {history[-1]:.4f}\")\n",
                "    return history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(y_true, y_pred, y_prob=None):\n",
                "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_true, y_pred),\n",
                "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
                "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
                "        'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
                "        'fpr': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
                "        'tpr': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
                "    }\n",
                "    if y_prob is not None and not np.any(np.isnan(y_prob)):\n",
                "        try:\n",
                "            metrics['roc_auc'] = roc_auc_score(y_true, y_prob)\n",
                "        except:\n",
                "            metrics['roc_auc'] = 0.5\n",
                "    else:\n",
                "        metrics['roc_auc'] = 0.5\n",
                "    return metrics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train and Evaluate Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_metrics = []\n",
                "all_histories = {}\n",
                "all_probabilities = {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Deep MLP\n",
                "print(\"Training Deep MLP...\")\n",
                "model = DeepMLP(input_dim)\n",
                "start_time = time.time()\n",
                "history = train_classifier(model, train_loader, epochs=100)\n",
                "train_time = time.time() - start_time\n",
                "\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    y_prob = model(torch.tensor(X_test).to(device)).cpu().numpy().flatten()\n",
                "    y_pred = (y_prob > 0.5).astype(int)\n",
                "inference_time = time.time() - start_time - train_time\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics.update({'model': 'Deep_MLP', 'train_time': train_time, 'inference_time': inference_time})\n",
                "all_metrics.append(metrics)\n",
                "all_histories['Deep_MLP'] = history\n",
                "all_probabilities['Deep_MLP'] = y_prob\n",
                "torch.save(model.state_dict(), MODELS_DIR / 'deep_mlp.pth')\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, ROC-AUC: {metrics['roc_auc']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Deep Autoencoder\n",
                "print(\"\\nTraining Deep Autoencoder...\")\n",
                "model = DeepAutoencoder(input_dim)\n",
                "start_time = time.time()\n",
                "history = train_autoencoder(model, normal_loader, epochs=100)\n",
                "train_time = time.time() - start_time\n",
                "\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    reconstructed = model(torch.tensor(X_test).to(device)).cpu().numpy()\n",
                "    reconstruction_error = np.mean((X_test - reconstructed) ** 2, axis=1)\n",
                "    train_reconstructed = model(torch.tensor(X_train_normal).to(device)).cpu().numpy()\n",
                "    train_error = np.mean((X_train_normal - train_reconstructed) ** 2, axis=1)\n",
                "    threshold = np.percentile(train_error, 95)\n",
                "    y_pred = (reconstruction_error > threshold).astype(int)\n",
                "    y_prob = safe_normalize(reconstruction_error)\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics.update({'model': 'Deep_Autoencoder', 'train_time': train_time, 'inference_time': 0.01})\n",
                "all_metrics.append(metrics)\n",
                "all_histories['Deep_Autoencoder'] = history\n",
                "all_probabilities['Deep_Autoencoder'] = y_prob\n",
                "torch.save(model.state_dict(), MODELS_DIR / 'deep_autoencoder.pth')\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, ROC-AUC: {metrics['roc_auc']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. VAE\n",
                "print(\"\\nTraining Variational Autoencoder...\")\n",
                "model = VAE(input_dim)\n",
                "start_time = time.time()\n",
                "history = train_vae(model, normal_loader, epochs=100)\n",
                "train_time = time.time() - start_time\n",
                "\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    reconstructed, _, _ = model(torch.tensor(X_test).to(device))\n",
                "    reconstruction_error = np.mean((X_test - reconstructed.cpu().numpy()) ** 2, axis=1)\n",
                "    train_reconstructed, _, _ = model(torch.tensor(X_train_normal).to(device))\n",
                "    train_error = np.mean((X_train_normal - train_reconstructed.cpu().numpy()) ** 2, axis=1)\n",
                "    threshold = np.percentile(train_error, 95)\n",
                "    y_pred = (reconstruction_error > threshold).astype(int)\n",
                "    y_prob = safe_normalize(reconstruction_error)\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics.update({'model': 'VAE', 'train_time': train_time, 'inference_time': 0.01})\n",
                "all_metrics.append(metrics)\n",
                "all_histories['VAE'] = history\n",
                "all_probabilities['VAE'] = y_prob\n",
                "torch.save(model.state_dict(), MODELS_DIR / 'vae.pth')\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, ROC-AUC: {metrics['roc_auc']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. LSTM Autoencoder\n",
                "print(\"\\nTraining LSTM Autoencoder...\")\n",
                "model = LSTMAutoencoder(input_dim)\n",
                "start_time = time.time()\n",
                "history = train_autoencoder(model, normal_loader, epochs=50)\n",
                "train_time = time.time() - start_time\n",
                "\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    reconstructed = model(torch.tensor(X_test).to(device)).cpu().numpy()\n",
                "    reconstruction_error = np.mean((X_test - reconstructed) ** 2, axis=1)\n",
                "    train_reconstructed = model(torch.tensor(X_train_normal).to(device)).cpu().numpy()\n",
                "    train_error = np.mean((X_train_normal - train_reconstructed) ** 2, axis=1)\n",
                "    threshold = np.percentile(train_error, 95)\n",
                "    y_pred = (reconstruction_error > threshold).astype(int)\n",
                "    y_prob = safe_normalize(reconstruction_error)\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics.update({'model': 'LSTM_Autoencoder', 'train_time': train_time, 'inference_time': 0.01})\n",
                "all_metrics.append(metrics)\n",
                "all_histories['LSTM_Autoencoder'] = history\n",
                "all_probabilities['LSTM_Autoencoder'] = y_prob\n",
                "torch.save(model.state_dict(), MODELS_DIR / 'lstm_autoencoder.pth')\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, ROC-AUC: {metrics['roc_auc']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Deep One-Class Neural Network\n",
                "print(\"\\nTraining Deep One-Class Neural Network...\")\n",
                "model = DeepOCNN(input_dim)\n",
                "model.to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    model.center.data = model(torch.tensor(X_train_normal[:100]).to(device)).mean(dim=0)\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "start_time = time.time()\n",
                "history = []\n",
                "for epoch in range(100):\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    for batch in normal_loader:\n",
                "        X_batch = batch[0].to(device)\n",
                "        optimizer.zero_grad()\n",
                "        loss = torch.mean(model.compute_score(X_batch))\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    history.append(epoch_loss / len(normal_loader))\n",
                "    if (epoch + 1) % 20 == 0:\n",
                "        print(f\"  Epoch {epoch+1}/100, Loss: {history[-1]:.4f}\")\n",
                "train_time = time.time() - start_time\n",
                "\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    scores = model.compute_score(torch.tensor(X_test).to(device)).cpu().numpy()\n",
                "    train_scores = model.compute_score(torch.tensor(X_train_normal).to(device)).cpu().numpy()\n",
                "    threshold = np.percentile(train_scores, 95)\n",
                "    y_pred = (scores > threshold).astype(int)\n",
                "    y_prob = safe_normalize(scores)\n",
                "\n",
                "metrics = compute_metrics(y_test, y_pred, y_prob)\n",
                "metrics.update({'model': 'Deep_OCNN', 'train_time': train_time, 'inference_time': 0.01})\n",
                "all_metrics.append(metrics)\n",
                "all_histories['Deep_OCNN'] = history\n",
                "all_probabilities['Deep_OCNN'] = y_prob\n",
                "torch.save(model.state_dict(), MODELS_DIR / 'deep_ocnn.pth')\n",
                "print(f\"  F1: {metrics['f1_score']:.4f}, ROC-AUC: {metrics['roc_auc']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "metrics_df = pd.DataFrame(all_metrics)\n",
                "col_order = ['model', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'fpr', 'tpr', 'train_time', 'inference_time']\n",
                "extra_cols = [c for c in metrics_df.columns if c not in col_order]\n",
                "metrics_df = metrics_df[col_order + extra_cols]\n",
                "\n",
                "metrics_path = RESULTS_DIR / 'neural_metrics.csv'\n",
                "metrics_df.to_csv(metrics_path, index=False)\n",
                "\n",
                "print(f\"✅ Saved neural metrics to: {metrics_path}\")\n",
                "print(\"\\nNeural Network Results:\")\n",
                "print(metrics_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training curves\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, (name, history) in enumerate(all_histories.items()):\n",
                "    ax = axes[i]\n",
                "    ax.plot(history)\n",
                "    ax.set_xlabel('Epoch')\n",
                "    ax.set_ylabel('Loss')\n",
                "    ax.set_title(f'{name} Training Loss')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "if len(all_histories) < 6:\n",
                "    axes[-1].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'neural_training_curves.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ROC Curves (with NaN handling)\n",
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "for model_name, y_prob in all_probabilities.items():\n",
                "    if y_prob is not None and not np.any(np.isnan(y_prob)):\n",
                "        try:\n",
                "            fpr_vals, tpr_vals, _ = roc_curve(y_test, y_prob)\n",
                "            auc_val = metrics_df[metrics_df['model'] == model_name]['roc_auc'].values[0]\n",
                "            plt.plot(fpr_vals, tpr_vals, label=f'{model_name} (AUC={auc_val:.3f})')\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {model_name}: {e}\")\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curves - Neural Network Models')\n",
                "plt.legend(loc='lower right')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'roc_curves_neural.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"NEURAL NETWORK MODELS SUMMARY\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Total models trained: {len(all_metrics)}\")\n",
                "print(f\"\\nBest by F1 Score: {metrics_df.loc[metrics_df['f1_score'].idxmax(), 'model']} ({metrics_df['f1_score'].max():.4f})\")\n",
                "print(f\"Best by ROC-AUC: {metrics_df.loc[metrics_df['roc_auc'].idxmax(), 'model']} ({metrics_df['roc_auc'].max():.4f})\")\n",
                "print(\"\\n✅ Notebook 7 Complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}