{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 1: Data Ingestion & Initial Processing\n",
                "\n",
                "**Purpose**: Load raw dataset, handle missing values, remove duplicates, perform basic sanity checks, and normalize labels.\n",
                "\n",
                "**Outputs**:\n",
                "- `cleaned_data.csv` → `data/processed/`\n",
                "- `data_summary.json` → `results/`\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "RANDOM_SEED = 42\n",
                "np.random.seed(RANDOM_SEED)\n",
                "\n",
                "# Paths\n",
                "BASE_DIR = Path('.').resolve().parent\n",
                "RAW_DATA_DIR = BASE_DIR / 'data' / 'raw'\n",
                "PROCESSED_DIR = BASE_DIR / 'data' / 'processed'\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "\n",
                "# Create directories if they don't exist\n",
                "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Base Directory: {BASE_DIR}\")\n",
                "print(f\"Raw Data Directory: {RAW_DATA_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Raw Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the credit card fraud dataset\n",
                "raw_file = RAW_DATA_DIR / 'creditcard.csv'\n",
                "\n",
                "print(f\"Loading data from: {raw_file}\")\n",
                "df_raw = pd.read_csv(raw_file)\n",
                "\n",
                "print(f\"\\nDataset Shape: {df_raw.shape}\")\n",
                "print(f\"Columns: {list(df_raw.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic info\n",
                "print(\"Dataset Info:\")\n",
                "print(\"=\" * 50)\n",
                "df_raw.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary\n",
                "df_raw.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Handle Missing Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "missing_counts = df_raw.isnull().sum()\n",
                "missing_pct = (missing_counts / len(df_raw)) * 100\n",
                "\n",
                "missing_df = pd.DataFrame({\n",
                "    'Missing Count': missing_counts,\n",
                "    'Missing %': missing_pct\n",
                "})\n",
                "\n",
                "print(\"Missing Values Summary:\")\n",
                "print(missing_df[missing_df['Missing Count'] > 0])\n",
                "\n",
                "if missing_df['Missing Count'].sum() == 0:\n",
                "    print(\"\\n✅ No missing values found!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle missing values (if any)\n",
                "df_clean = df_raw.dropna()\n",
                "rows_dropped = len(df_raw) - len(df_clean)\n",
                "print(f\"Rows dropped due to missing values: {rows_dropped}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Remove Duplicates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for duplicates\n",
                "duplicates_count = df_clean.duplicated().sum()\n",
                "print(f\"Duplicate rows found: {duplicates_count}\")\n",
                "\n",
                "# Remove duplicates\n",
                "df_clean = df_clean.drop_duplicates()\n",
                "print(f\"Shape after removing duplicates: {df_clean.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Basic Sanity Checks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target column analysis\n",
                "TARGET_COLUMN = 'Class'\n",
                "\n",
                "print(\"Target Column Distribution (Full Dataset):\")\n",
                "print(\"=\" * 50)\n",
                "target_counts = df_clean[TARGET_COLUMN].value_counts()\n",
                "target_pct = df_clean[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
                "\n",
                "print(f\"Normal (0): {target_counts[0]:,} ({target_pct[0]:.4f}%)\")\n",
                "print(f\"Fraud (1):  {target_counts[1]:,} ({target_pct[1]:.4f}%)\")\n",
                "print(f\"\\nClass Imbalance Ratio: 1:{target_counts[0]/target_counts[1]:.1f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize class distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Bar plot\n",
                "colors = ['#2ecc71', '#e74c3c']\n",
                "ax1 = axes[0]\n",
                "ax1.bar(['Normal', 'Fraud'], [target_counts[0], target_counts[1]], color=colors)\n",
                "ax1.set_ylabel('Count')\n",
                "ax1.set_title('Class Distribution (Full Dataset)')\n",
                "ax1.set_yscale('log')\n",
                "\n",
                "# Pie chart\n",
                "ax2 = axes[1]\n",
                "ax2.pie([target_counts[0], target_counts[1]], labels=['Normal', 'Fraud'], \n",
                "        autopct='%1.2f%%', colors=colors, explode=[0, 0.1])\n",
                "ax2.set_title('Class Distribution (%)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR.parent / 'figures' / 'class_distribution_raw.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data type checks\n",
                "print(\"Data Types:\")\n",
                "print(df_clean.dtypes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Stratified Subsampling (2000 samples with ~5% anomalies)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "MAX_SAMPLES = 2000\n",
                "TARGET_ANOMALY_RATIO = 0.05  # 5% anomalies\n",
                "\n",
                "# Calculate sample sizes\n",
                "n_anomalies = int(MAX_SAMPLES * TARGET_ANOMALY_RATIO)  # 100 anomalies\n",
                "n_normal = MAX_SAMPLES - n_anomalies  # 1900 normal\n",
                "\n",
                "print(f\"Target samples: {MAX_SAMPLES}\")\n",
                "print(f\"Normal samples: {n_normal}\")\n",
                "print(f\"Anomaly samples: {n_anomalies}\")\n",
                "print(f\"Target anomaly ratio: {TARGET_ANOMALY_RATIO*100}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate classes\n",
                "df_normal = df_clean[df_clean[TARGET_COLUMN] == 0]\n",
                "df_fraud = df_clean[df_clean[TARGET_COLUMN] == 1]\n",
                "\n",
                "print(f\"Available normal samples: {len(df_normal)}\")\n",
                "print(f\"Available fraud samples: {len(df_fraud)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stratified sampling\n",
                "np.random.seed(RANDOM_SEED)\n",
                "\n",
                "# Sample normal class\n",
                "df_normal_sampled = df_normal.sample(n=n_normal, random_state=RANDOM_SEED)\n",
                "\n",
                "# Sample fraud class (use all if less than required, else sample)\n",
                "if len(df_fraud) <= n_anomalies:\n",
                "    df_fraud_sampled = df_fraud.copy()\n",
                "    print(f\"⚠️ Using all {len(df_fraud)} fraud samples (less than target {n_anomalies})\")\n",
                "else:\n",
                "    df_fraud_sampled = df_fraud.sample(n=n_anomalies, random_state=RANDOM_SEED)\n",
                "\n",
                "# Combine\n",
                "df_subsampled = pd.concat([df_normal_sampled, df_fraud_sampled], ignore_index=True)\n",
                "\n",
                "# Shuffle\n",
                "df_subsampled = df_subsampled.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
                "\n",
                "print(f\"\\nSubsampled dataset shape: {df_subsampled.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify subsample distribution\n",
                "print(\"Subsampled Dataset Distribution:\")\n",
                "print(\"=\" * 50)\n",
                "sub_counts = df_subsampled[TARGET_COLUMN].value_counts()\n",
                "sub_pct = df_subsampled[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
                "\n",
                "print(f\"Normal (0): {sub_counts[0]:,} ({sub_pct[0]:.2f}%)\")\n",
                "print(f\"Fraud (1):  {sub_counts[1]:,} ({sub_pct[1]:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Label Normalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensure labels are 0 and 1\n",
                "assert set(df_subsampled[TARGET_COLUMN].unique()) == {0, 1}, \"Labels should be 0 and 1\"\n",
                "print(\"✅ Labels are already normalized (0 = Normal, 1 = Anomaly)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save cleaned data\n",
                "output_path = PROCESSED_DIR / 'cleaned_data.csv'\n",
                "df_subsampled.to_csv(output_path, index=False)\n",
                "print(f\"✅ Saved cleaned data to: {output_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and save data summary\n",
                "# Convert numpy types to native Python types for JSON serialization\n",
                "data_summary = {\n",
                "    \"source_file\": str(raw_file),\n",
                "    \"original_shape\": list(df_raw.shape),\n",
                "    \"cleaned_shape\": list(df_subsampled.shape),\n",
                "    \"random_seed\": int(RANDOM_SEED),\n",
                "    \"target_column\": TARGET_COLUMN,\n",
                "    \"class_distribution\": {\n",
                "        \"normal_count\": int(sub_counts[0]),\n",
                "        \"anomaly_count\": int(sub_counts[1]),\n",
                "        \"normal_percentage\": float(round(sub_pct[0], 2)),\n",
                "        \"anomaly_percentage\": float(round(sub_pct[1], 2))\n",
                "    },\n",
                "    \"features\": {\n",
                "        \"total\": int(len(df_subsampled.columns) - 1),\n",
                "        \"names\": [col for col in df_subsampled.columns if col != TARGET_COLUMN]\n",
                "    },\n",
                "    \"missing_values_dropped\": int(rows_dropped),\n",
                "    \"duplicates_dropped\": int(duplicates_count),\n",
                "    \"preprocessing_steps\": [\n",
                "        \"Loaded raw creditcard.csv\",\n",
                "        \"Dropped missing values\",\n",
                "        \"Removed duplicates\",\n",
                "        f\"Stratified subsampling to {MAX_SAMPLES} samples\",\n",
                "        f\"Target anomaly ratio: {TARGET_ANOMALY_RATIO*100}%\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "summary_path = RESULTS_DIR / 'data_summary.json'\n",
                "with open(summary_path, 'w') as f:\n",
                "    json.dump(data_summary, f, indent=2)\n",
                "\n",
                "print(f\"✅ Saved data summary to: {summary_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display summary\n",
                "print(\"\\nData Summary:\")\n",
                "print(\"=\" * 50)\n",
                "print(json.dumps(data_summary, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify saved file\n",
                "df_verify = pd.read_csv(PROCESSED_DIR / 'cleaned_data.csv')\n",
                "print(f\"Verification - Loaded shape: {df_verify.shape}\")\n",
                "print(f\"Verification - Class distribution: {df_verify[TARGET_COLUMN].value_counts().to_dict()}\")\n",
                "print(\"\\n✅ Notebook 1 Complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}